{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply optimizations to ONNX model\n",
    "\n",
    "Now that we have an ONNX model, we can apply some basic optimizations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from utilities import pad_or_truncate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test dataset\n",
    "SEQ_LEN = 50  # 或模型导出时的 seq_max_len\n",
    "class MovieLensTestDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.data = self.df.to_dict(orient=\"records\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data[idx]\n",
    "        user_id = int(row[\"user_id\"])\n",
    "        sequence = eval(row[\"sequence\"]) if isinstance(row[\"sequence\"], str) else row[\"sequence\"]\n",
    "        sequence = pad_or_truncate(sequence, SEQ_LEN)\n",
    "        return user_id, sequence\n",
    "\n",
    "movielens_data_dir = os.getenv(\"MOVIELENS_DATA_DIR\", \"/mnt/data\")\n",
    "test_dataset = MovieLensTestDataset(os.path.join(movielens_data_dir, \"test.csv\"))\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_session(ort_session):\n",
    "    print(f\"Execution provider: {ort_session.get_providers()}\")\n",
    "\n",
    "    user_input_name = ort_session.get_inputs()[0].name\n",
    "    seq_input_name = ort_session.get_inputs()[1].name\n",
    "\n",
    "    # Benchmark accuracy\n",
    "    correct, total = 0, 0\n",
    "    for user_ids, sequences in test_loader:\n",
    "        u = np.array(user_ids)\n",
    "        s = np.stack(sequences)\n",
    "        outputs = ort_session.run(None, {user_input_name: u, seq_input_name: s})[0]\n",
    "        preds = np.argmax(outputs, axis=1)\n",
    "        total += len(preds)\n",
    "        correct += len(preds)  \n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Accuracy (dummy): {accuracy:.2f}%\")\n",
    "\n",
    "    # Benchmark inference latency \n",
    "    user, seq = test_dataset[0]\n",
    "    u = np.array([user])\n",
    "    s = np.array([seq])\n",
    "\n",
    "    ort_session.run(None, {user_input_name: u, seq_input_name: s})  # warmup\n",
    "\n",
    "    latencies = []\n",
    "    for _ in range(100):\n",
    "        start = time.time()\n",
    "        ort_session.run(None, {user_input_name: u, seq_input_name: s})\n",
    "        latencies.append(time.time() - start)\n",
    "\n",
    "    print(f\"Inference Latency (median): {np.percentile(latencies, 50)*1000:.2f} ms\")\n",
    "    print(f\"Inference Latency (95th): {np.percentile(latencies, 95)*1000:.2f} ms\")\n",
    "    print(f\"Inference Latency (99th): {np.percentile(latencies, 99)*1000:.2f} ms\")\n",
    "    print(f\"Inference Throughput (single sample): {100/np.sum(latencies):.2f} FPS\")\n",
    "\n",
    "    #  Benchmark batch throughput\n",
    "    user_ids, sequences = next(iter(test_loader))\n",
    "    u = np.array(user_ids)\n",
    "    s = np.stack(sequences)\n",
    "\n",
    "    ort_session.run(None, {user_input_name: u, seq_input_name: s})  # warmup\n",
    "\n",
    "    batch_times = []\n",
    "    for _ in range(50):\n",
    "        start = time.time()\n",
    "        ort_session.run(None, {user_input_name: u, seq_input_name: s})\n",
    "        batch_times.append(time.time() - start)\n",
    "\n",
    "    batch_fps = (len(user_ids) * 50) / np.sum(batch_times)\n",
    "    print(f\"Batch Throughput: {batch_fps:.2f} FPS\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply basic graph optimizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = \"models/SSE_PT10kemb.onnx\"\n",
    "optimized_model_path = \"models/SSE_PT10kemb_optimized.onnx\"\n",
    "\n",
    "session_options = ort.SessionOptions()\n",
    "session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n",
    "session_options.optimized_model_filepath = optimized_model_path\n",
    "\n",
    "ort_session = ort.InferenceSession(\n",
    "    onnx_model_path,\n",
    "    sess_options=session_options,\n",
    "    providers=['CPUExecutionProvider']\n",
    ")\n",
    "\n",
    "print(f\"Optimized ONNX model saved to {optimized_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_session = ort.InferenceSession(optimized_model_path, providers=['CPUExecutionProvider'])\n",
    "benchmark_session(optimized_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neural_compressor\n",
    "from neural_compressor import quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp32_model_path = \"models/SSE_PT10kemb.onnx\"\n",
    "model = ONNXModel(fp32_model_path)\n",
    "\n",
    "config = PostTrainingQuantConfig(\n",
    "    approach=\"dynamic\"\n",
    ")\n",
    "\n",
    "q_model = quantization.fit(\n",
    "    model=model,\n",
    "    conf=config\n",
    ")\n",
    "\n",
    "quantized_model_path = \"models/SSE_PT10kemb_quant_dynamic.onnx\"\n",
    "q_model.save_model_to_file(quantized_model_path)\n",
    "print(f\"Quantized model saved to {quantized_model_path}\")\n",
    "\n",
    "# compare\n",
    "model_size = os.path.getsize(quantized_model_path)\n",
    "print(f\"Quantized Model Size on Disk: {model_size / 1e6:.2f} MB\")\n",
    "\n",
    "ort_session = ort.InferenceSession(quantized_model_path, providers=['CPUExecutionProvider'])\n",
    "benchmark_session(ort_session)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Static quantization\n",
    "\n",
    "Next, we will try static quantization with a calibration dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neural_compressor\n",
    "from neural_compressor import quantization\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 50\n",
    "class MovieLensTestDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.data = self.df.to_dict(orient=\"records\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data[idx]\n",
    "        user_id = int(row[\"user_id\"])\n",
    "        sequence = eval(row[\"sequence\"]) if isinstance(row[\"sequence\"], str) else row[\"sequence\"]\n",
    "        sequence = pad_or_truncate(sequence, SEQ_LEN)\n",
    "        return {\"user\": user_id, \"sequence\": sequence}\n",
    "\n",
    "movielens_data_dir = os.getenv(\"MOVIELENS_DATA_DIR\", \"/mnt/data\")\n",
    "val_dataset = MovieLensTestDataset(os.path.join(movielens_data_dir, \"validation.csv\"))\n",
    "eval_dataloader = INCDataLoader(framework=\"onnxruntime\", dataset=val_dataset)\n",
    "\n",
    "fp32_model_path = \"models/SSE_PT10kemb.onnx\"\n",
    "model = ONNXModel(fp32_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Aggressive Static Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_aggressive = PostTrainingQuantConfig(\n",
    "    accuracy_criterion=AccuracyCriterion(\n",
    "        criterion=\"absolute\",\n",
    "        tolerable_loss=0.05\n",
    "    ),\n",
    "    approach=\"static\",\n",
    "    device=\"cpu\",\n",
    "    quant_level=1,\n",
    "    quant_format=\"QOperator\",\n",
    "    recipes={\"graph_optimization_level\": \"ENABLE_EXTENDED\"},\n",
    "    calibration_sampling_size=128\n",
    ")\n",
    "\n",
    "q_model_aggressive = quantization.fit(\n",
    "    model=model,\n",
    "    conf=config_aggressive,\n",
    "    calib_dataloader=eval_dataloader,\n",
    "    eval_dataloader=eval_dataloader,\n",
    "    eval_metric=Metric(name=\"topk\")  # 可用 top1/top5\n",
    ")\n",
    "\n",
    "quant_model_path = \"models/SSE_PT10kemb_quant_static_aggressive.onnx\"\n",
    "q_model_aggressive.save_model_to_file(quant_model_path)\n",
    "print(f\"Aggressively Quantized model saved to {quant_model_path}\")\n",
    "print(f\"Model Size: {os.path.getsize(quant_model_path) / 1e6:.2f} MB\")\n",
    "\n",
    "# Benchmark\n",
    "ort_session = ort.InferenceSession(quant_model_path, providers=[\"CPUExecutionProvider\"])\n",
    "benchmark_session(ort_session)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Conservative Static Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_conservative = PostTrainingQuantConfig(\n",
    "    accuracy_criterion=AccuracyCriterion(\n",
    "        criterion=\"absolute\",\n",
    "        tolerable_loss=0.01\n",
    "    ),\n",
    "    approach=\"static\",\n",
    "    device=\"cpu\",\n",
    "    quant_level=0,\n",
    "    quant_format=\"QOperator\",\n",
    "    recipes={\"graph_optimization_level\": \"ENABLE_EXTENDED\"},\n",
    "    calibration_sampling_size=128\n",
    ")\n",
    "\n",
    "q_model_conservative = quantization.fit(\n",
    "    model=model,\n",
    "    conf=config_conservative,\n",
    "    calib_dataloader=eval_dataloader,\n",
    "    eval_dataloader=eval_dataloader,\n",
    "    eval_metric=Metric(name=\"topk\")\n",
    ")\n",
    "\n",
    "quant_model_path = \"models/SSE_PT10kemb_quant_static_conservative.onnx\"\n",
    "q_model_conservative.save_model_to_file(quant_model_path)\n",
    "print(f\"Conservatively Quantized model saved to {quant_model_path}\")\n",
    "print(f\"Model Size: {os.path.getsize(quant_model_path) / 1e6:.2f} MB\")\n",
    "\n",
    "# Benchmark\n",
    "ort_session = ort.InferenceSession(quant_model_path, providers=[\"CPUExecutionProvider\"])\n",
    "benchmark_session(ort_session)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
