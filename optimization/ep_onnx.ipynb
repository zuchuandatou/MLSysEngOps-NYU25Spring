{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try a different execution provider\n",
    "\n",
    "Once a model is in ONNX format, we can use it with many *execution providers*. In ONNX, an execution provider an interface that lets ONNX models run with special hardware-specific capabilities. Until now, we have been using the `CPUExecutionProvider`, but if we use hardware-specific capabilities, e.g. switch out generic implementations of graph operations for implementations that are optimized for specific hardware, we can execute exactly the same model, much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import onnxruntime as ort\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utilities import pad_or_truncate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import onnxruntime as ort\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utilities import pad_or_truncate\n",
    "\n",
    "SEQ_LEN = 50\n",
    "class MovieLensTestDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.data = self.df.to_dict(orient=\"records\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data[idx]\n",
    "        user_id = int(row[\"user_id\"])\n",
    "        sequence = eval(row[\"sequence\"]) if isinstance(row[\"sequence\"], str) else row[\"sequence\"]\n",
    "        sequence = pad_or_truncate(sequence, SEQ_LEN)\n",
    "        return user_id, sequence\n",
    "\n",
    "movielens_data_dir = os.getenv(\"MOVIELENS_DATA_DIR\", \"/mnt/data\")\n",
    "test_dataset = MovieLensTestDataset(os.path.join(movielens_data_dir, \"test.csv\"))\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_session(ort_session):\n",
    "    user_input = ort_session.get_inputs()[0].name\n",
    "    seq_input = ort_session.get_inputs()[1].name\n",
    "\n",
    "    print(f\"Execution provider: {ort_session.get_providers()}\")\n",
    "\n",
    "    correct, total = 0, 0\n",
    "    for user_ids, sequences in test_loader:\n",
    "        u = np.array(user_ids)\n",
    "        s = np.stack(sequences)\n",
    "        outputs = ort_session.run(None, {user_input: u, seq_input: s})[0]\n",
    "        preds = np.argmax(outputs, axis=1)\n",
    "        total += len(preds)\n",
    "        correct += len(preds)  # 假设无 label\n",
    "\n",
    "    print(f\"Accuracy (dummy): {correct / total * 100:.2f}%\")\n",
    "\n",
    "    # Benchmark inference latency for single sample\n",
    "    u, s = test_dataset[0]\n",
    "    u = np.array([u])\n",
    "    s = np.array([s])\n",
    "    ort_session.run(None, {user_input: u, seq_input: s})  # warmup\n",
    "    latencies = []\n",
    "    for _ in range(100):\n",
    "        import time\n",
    "        start = time.time()\n",
    "        ort_session.run(None, {user_input: u, seq_input: s})\n",
    "        latencies.append(time.time() - start)\n",
    "    print(f\"Latency median: {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "    print(f\"Latency 95th: {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "    print(f\"Latency 99th: {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "    print(f\"Inference Throughput: {100 / np.sum(latencies):.2f} FPS\")\n",
    "\n",
    "    # Benchmark batch throughput\n",
    "    batch_users, batch_seqs = next(iter(test_loader))\n",
    "    u = np.array(batch_users)\n",
    "    s = np.stack(batch_seqs)\n",
    "    ort_session.run(None, {user_input: u, seq_input: s})\n",
    "    batch_times = []\n",
    "    for _ in range(50):\n",
    "        start = time.time()\n",
    "        ort_session.run(None, {user_input: u, seq_input: s})\n",
    "        batch_times.append(time.time() - start)\n",
    "    print(f\"Batch Throughput: {(len(u) * 50) / np.sum(batch_times):.2f} FPS\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "Execution provider: ['CPUExecutionProvider']\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 9.93 ms\n",
    "Inference Latency (single sample, 95th percentile): 14.20 ms\n",
    "Inference Latency (single sample, 99th percentile): 14.43 ms\n",
    "Inference Throughput (single sample): 91.10 FPS\n",
    "Batch Throughput: 1042.47 FPS\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try different Execution Provider "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "Execution provider: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 0.89 ms\n",
    "Inference Latency (single sample, 95th percentile): 0.90 ms\n",
    "Inference Latency (single sample, 99th percentile): 0.91 ms\n",
    "Inference Throughput (single sample): 1117.06 FPS\n",
    "Batch Throughput: 5181.99 FPS\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CPUExecutionProvider\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = \"models/SSE_PT10kemb_quant_static_conservative.onnx\"  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[CPUExecutionProvider]\")\n",
    "session_cpu = ort.InferenceSession(onnx_model_path, providers=[\"CPUExecutionProvider\"])\n",
    "benchmark_session(session_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  OpenVINO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"OpenVINOExecutionProvider\" in ort.get_available_providers():\n",
    "    print(\"\\n[OpenVINOExecutionProvider]\")\n",
    "    session_ov = ort.InferenceSession(onnx_model_path, providers=[\"OpenVINOExecutionProvider\"])\n",
    "    benchmark_session(session_ov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorRT execution provider\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"TensorrtExecutionProvider\" in ort.get_available_providers():\n",
    "    print(\"\\n[TensorrtExecutionProvider]\")\n",
    "    session_trt = ort.InferenceSession(onnx_model_path, providers=[\"TensorrtExecutionProvider\"])\n",
    "    benchmark_session(session_trt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "Execution provider: ['TensorrtExecutionProvider', 'CPUExecutionProvider']\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 0.63 ms\n",
    "Inference Latency (single sample, 95th percentile): 0.64 ms\n",
    "Inference Latency (single sample, 99th percentile): 0.70 ms\n",
    "Inference Throughput (single sample): 1572.61 FPS\n",
    "Batch Throughput: 9274.45 FPS\n",
    "-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
