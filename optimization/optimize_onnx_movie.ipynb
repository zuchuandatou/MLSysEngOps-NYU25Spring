{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply optimizations to ONNX model\n",
    "\n",
    "Now that we have an ONNX model, we can apply some basic optimizations. After completing this section, you should be able to apply:\n",
    "\n",
    "-   graph optimizations, e.g. fusing operations\n",
    "-   post-training quantization (dynamic and static)\n",
    "-   and hardware-specific execution providers\n",
    "\n",
    "to improve inference performance.\n",
    "\n",
    "You will execute this notebook *in a Jupyter container running on a compute instance*, not on the general-purpose Chameleon Jupyter environment from which you provision resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are going to evaluate several models, we’ll define a benchmark function here to help us compare them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test dataset\n",
    "food_11_data_dir = os.getenv(\"FOOD11_DATA_DIR\", \"Food-11\")\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "test_dataset = datasets.ImageFolder(root=os.path.join(food_11_data_dir, 'evaluation'), transform=val_test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_session(ort_session):\n",
    "\n",
    "    print(f\"Execution provider: {ort_session.get_providers()}\")\n",
    "\n",
    "    ## Benchmark accuracy\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images_np = images.numpy()\n",
    "        outputs = ort_session.run(None, {ort_session.get_inputs()[0].name: images_np})[0]\n",
    "        predicted = np.argmax(outputs, axis=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.numpy()).sum()\n",
    "    accuracy = (correct / total) * 100\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.2f}% ({correct}/{total} correct)\")\n",
    "\n",
    "    ## Benchmark inference latency for single sample\n",
    "\n",
    "    num_trials = 100  # Number of trials\n",
    "\n",
    "    # Get a single sample from the test data\n",
    "\n",
    "    single_sample, _ = next(iter(test_loader))  \n",
    "    single_sample = single_sample[:1].numpy()\n",
    "\n",
    "    # Warm-up run\n",
    "    ort_session.run(None, {ort_session.get_inputs()[0].name: single_sample})\n",
    "\n",
    "    latencies = []\n",
    "    for _ in range(num_trials):\n",
    "        start_time = time.time()\n",
    "        ort_session.run(None, {ort_session.get_inputs()[0].name: single_sample})\n",
    "        latencies.append(time.time() - start_time)\n",
    "\n",
    "    print(f\"Inference Latency (single sample, median): {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "    print(f\"Inference Latency (single sample, 95th percentile): {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "    print(f\"Inference Latency (single sample, 99th percentile): {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "    print(f\"Inference Throughput (single sample): {num_trials/np.sum(latencies):.2f} FPS\")\n",
    "\n",
    "    ## Benchmark batch throughput\n",
    "\n",
    "    num_batches = 50  # Number of trials\n",
    "\n",
    "    # Get a batch from the test data\n",
    "    batch_input, _ = next(iter(test_loader))  \n",
    "    batch_input = batch_input.numpy()\n",
    "\n",
    "    # Warm-up run\n",
    "    ort_session.run(None, {ort_session.get_inputs()[0].name: batch_input})\n",
    "\n",
    "    batch_times = []\n",
    "    for _ in range(num_batches):\n",
    "        start_time = time.time()\n",
    "        ort_session.run(None, {ort_session.get_inputs()[0].name: batch_input})\n",
    "        batch_times.append(time.time() - start_time)\n",
    "\n",
    "    batch_fps = (batch_input.shape[0] * num_batches) / np.sum(batch_times) \n",
    "    print(f\"Batch Throughput: {batch_fps:.2f} FPS\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply basic graph optimizations\n",
    "\n",
    "Let’s start by applying some basic [graph optimizations](https://onnxruntime.ai/docs/performance/model-optimizations/graph-optimizations.html#onlineoffline-mode), e.g. fusing operations.\n",
    "\n",
    "We will save the model after applying graph optimizations to `models/food11_optimized.onnx`, then evaluate that model in a new session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = \"models/food11.onnx\"\n",
    "optimized_model_path = \"models/food11_optimized.onnx\"\n",
    "\n",
    "session_options = ort.SessionOptions()\n",
    "session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED # apply graph optimizations\n",
    "session_options.optimized_model_filepath = optimized_model_path \n",
    "\n",
    "ort_session = ort.InferenceSession(onnx_model_path, sess_options=session_options, providers=['CPUExecutionProvider'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the `food11_optimized.onnx` model from inside the `models` directory.\n",
    "\n",
    "To see the effect of the graph optimizations, we can visualize the models using [Netron](https://netron.app/). Upload the original `food11.onnx` and review the graph. Then, upload the `food11_optimized.onnx` and see what has changed in the “optimized” graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, evaluate the optimized model. The graph optimizations may improve the inference performance, may have negligible effect, OR they can make it worse, depending on the model and the hardware environment in which the model is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = \"models/food11_optimized.onnx\"\n",
    "ort_session = ort.InferenceSession(onnx_model_path, providers=['CPUExecutionProvider'])\n",
    "benchmark_session(ort_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "\n",
    "On gigaio AMD EPYC:\n",
    "\n",
    "\n",
    "Execution provider: ['CPUExecutionProvider']\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 8.70 ms\n",
    "Inference Latency (single sample, 95th percentile): 8.88 ms\n",
    "Inference Latency (single sample, 99th percentile): 9.24 ms\n",
    "Inference Throughput (single sample): 114.63 FPS\n",
    "Batch Throughput: 1153.63 FPS\n",
    "\n",
    "On liqid Intel:\n",
    "\n",
    "Execution provider: ['CPUExecutionProvider']\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 4.63 ms\n",
    "Inference Latency (single sample, 95th percentile): 4.67 ms\n",
    "Inference Latency (single sample, 99th percentile): 4.75 ms\n",
    "Inference Throughput (single sample): 214.45 FPS\n",
    "Batch Throughput: 2488.54 FPS\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply post training quantization\n",
    "\n",
    "We will continue our quest to improve inference speed! The next optimization we will attempt is quantization.\n",
    "\n",
    "There are many frameworks that offer quantization - for our Food11 model, we could:\n",
    "\n",
    "-   use [PyTorch quantization](https://pytorch.org/docs/stable/quantization.html#introduction-to-quantization)\n",
    "-   use [ONNX quantization](https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html)\n",
    "-   use [Intel Neural Compressor](https://intel.github.io/neural-compressor/latest/index.html) (which supports PyTorch and ONNX models)\n",
    "-   use [NNCF](https://github.com/openvinotoolkit/nncf) if we plan to use the OpenVINO execution provider\n",
    "-   etc…\n",
    "\n",
    "These frameworks vary in the type of quantization they support, the range of operations that may be quantized, and many other details.\n",
    "\n",
    "We will use Intel Neural Compressor, which in addition to supporting many ML frameworks and many types of quantization has an interesting feature: it supports quantization up to a specified evaluation threshold. In other words, we can specify “quantize as much as possible, but without losing more than 0.01 accuracy” and Intel Neural Compressor will find the best quantized version of the model that does not lose more than 0.01 accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post-training quantization comes in two main types. In both types, FP32 values will be converted in INT8, using\n",
    "\n",
    "$$X_{\\text{INT8}} = \\text{round} ( \\text{scale}  \\times X_{\\text{FP32}} + \\text{zero\\_point} )$$\n",
    "\n",
    "but they differ with respect to when and how the quantization parameters “scale” and “zero point” are computed:\n",
    "\n",
    "-   dynamic quantization: weights are quantized in advance and stored in INT8 representation. The quantization parameters for the activations are computed during inference.\n",
    "-   static quantization: weights are quantized in advance and stored in INT8, and the quantization parameters are also set in advance for activations. This approach requires the use of a “calibration dataset” during quantization, to set the quantization parameters for the activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamic quantization\n",
    "\n",
    "We will start with dynamic quantization. No calibration dataset is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neural_compressor\n",
    "from neural_compressor import quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ONNX model into Intel Neural Compressor\n",
    "model_path = \"models/food11.onnx\"\n",
    "fp32_model = neural_compressor.model.onnx_model.ONNXModel(model_path)\n",
    "\n",
    "# Configure the quantizer\n",
    "config_ptq = neural_compressor.PostTrainingQuantConfig(\n",
    "    approach=\"dynamic\"\n",
    ")\n",
    "\n",
    "# Fit the quantized model\n",
    "q_model = quantization.fit(\n",
    "    model=fp32_model, \n",
    "    conf=config_ptq\n",
    ")\n",
    "\n",
    "# Save quantized model\n",
    "q_model.save_model_to_file(\"models/food11_quantized_dynamic.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the `food11_quantized_dynamic.onnx` model from inside the `models` directory.\n",
    "\n",
    "To see the effect of the graph optimizations, we can visualize the models using [Netron](https://netron.app/). Upload the original `food11.onnx` and review the graph. Then, upload the `food11_quantized_dynamic.onnx` and see what has changed in the quantized graph.\n",
    "\n",
    "Note that some of our operations have become integer operations, but we have added additional operations to quantize and dequantize activations throughout the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also concerned with the size of the quantized model on disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = \"models/food11_quantized_dynamic.onnx\"\n",
    "model_size = os.path.getsize(onnx_model_path) \n",
    "print(f\"Model Size on Disk: {model_size/ (1e6) :.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, evaluate the quantized model. Since we are saving weights in integer form, the model size is smaller. With respect to inference time, however, while the integer operations may be faster than their FP32 equivalents, the dynamic quantization and dequantization of activations may add more compute time than we save from integer operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = \"models/food11_quantized_dynamic.onnx\"\n",
    "ort_session = ort.InferenceSession(onnx_model_path, providers=['CPUExecutionProvider'])\n",
    "benchmark_session(ort_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- \n",
    "\n",
    "On liqid AMD EPYC\n",
    "\n",
    "Model Size on Disk: 2.42 MB\n",
    "Execution provider: ['CPUExecutionProvider']\n",
    "Accuracy: 82.04% (2746/3347 correct)\n",
    "Inference Latency (single sample, median): 22.32 ms\n",
    "Inference Latency (single sample, 95th percentile): 22.97 ms\n",
    "Inference Latency (single sample, 99th percentile): 23.14 ms\n",
    "Inference Throughput (single sample): 44.71 FPS\n",
    "Batch Throughput: 38.34 FPS\n",
    "\n",
    "On liqid Intel\n",
    "\n",
    "Execution provider: ['CPUExecutionProvider']\n",
    "Accuracy: 84.58% (2831/3347 correct)\n",
    "Inference Latency (single sample, median): 28.29 ms\n",
    "Inference Latency (single sample, 95th percentile): 29.00 ms\n",
    "Inference Latency (single sample, 99th percentile): 29.07 ms\n",
    "Inference Throughput (single sample): 35.28 FPS\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Static quantization\n",
    "\n",
    "Next, we will try static quantization with a calibration dataset.\n",
    "\n",
    "First, let’s prepare the calibration dataset. This dataset will also be used to evaluate the quantized model, to see if it meets the accuracy criterion we will set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neural_compressor\n",
    "from neural_compressor import quantization\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_11_data_dir = os.getenv(\"FOOD11_DATA_DIR\", \"Food-11\")\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "val_dataset = datasets.ImageFolder(root=os.path.join(food_11_data_dir, 'validation'), transform=val_test_transform)\n",
    "eval_dataloader = neural_compressor.data.DataLoader(framework='onnxruntime', dataset=val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we’ll configure the quantizer. We’ll start with a more aggressive quantization strategy - we will prefer to quantize as much as possible, as long as the accuracy of the quantized model is not more than **0.05** less than the accuracy of the original FP32 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ONNX model into Intel Neural Compressor\n",
    "model_path = \"models/food11.onnx\"\n",
    "fp32_model = neural_compressor.model.onnx_model.ONNXModel(model_path)\n",
    "\n",
    "# Configure the quantizer\n",
    "config_ptq = neural_compressor.PostTrainingQuantConfig(\n",
    "    accuracy_criterion = neural_compressor.config.AccuracyCriterion(\n",
    "        criterion=\"absolute\",  \n",
    "        tolerable_loss=0.05  # We will tolerate up to 0.05 less accuracy in the quantized model\n",
    "    ),\n",
    "    approach=\"static\", \n",
    "    device='cpu', \n",
    "    quant_level=1,\n",
    "    quant_format=\"QOperator\", \n",
    "    recipes={\"graph_optimization_level\": \"ENABLE_EXTENDED\"}, \n",
    "    calibration_sampling_size=128\n",
    ")\n",
    "\n",
    "# Find the best quantized model meeting the accuracy criterion\n",
    "q_model = quantization.fit(\n",
    "    model=fp32_model, \n",
    "    conf=config_ptq, \n",
    "    calib_dataloader=eval_dataloader,\n",
    "    eval_dataloader=eval_dataloader, \n",
    "    eval_metric=neural_compressor.metric.Metric(name='topk')\n",
    ")\n",
    "\n",
    "# Save quantized model\n",
    "q_model.save_model_to_file(\"models/food11_quantized_aggressive.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the `food11_quantized_aggressive.onnx` model from inside the `models` directory.\n",
    "\n",
    "To see the effect of the graph optimizations, we can visualize the models using [Netron](https://netron.app/). Upload the original `food11.onnx` and review the graph. Then, upload the `food11_quantized_aggressive.onnx` and see what has changed in the quantized graph.\n",
    "\n",
    "Note that within the parameters for each quantized operation, we now have a “scale” and “zero point” - these are used to convert the FP32 values to INT8 values, as described above. The optimal scale and zero point for weights is determined by the fitted weights themselves, but the calibration dataset was required to find the optimal scale and zero point for activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s get the size of the quantized model on disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = \"models/food11_quantized_aggressive.onnx\"\n",
    "model_size = os.path.getsize(onnx_model_path) \n",
    "print(f\"Model Size on Disk: {model_size/ (1e6) :.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, evaluate the quantized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = \"models/food11_quantized_aggressive.onnx\"\n",
    "ort_session = ort.InferenceSession(onnx_model_path, providers=['CPUExecutionProvider'])\n",
    "benchmark_session(ort_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- \n",
    "\n",
    "On AMD EPYC\n",
    "\n",
    "Model Size on Disk: 2.42 MB\n",
    "Accuracy: 87.12% (2916/3347 correct)\n",
    "Inference Latency (single sample, median): 7.52 ms\n",
    "Inference Latency (single sample, 95th percentile): 7.78 ms\n",
    "Inference Latency (single sample, 99th percentile): 7.84 ms\n",
    "Inference Throughput (single sample): 132.40 FPS\n",
    "Batch Throughput: 899.98 FPS\n",
    "\n",
    "Model Size on Disk: 2.42 MB\n",
    "Accuracy: 87.12% (2916/3347 correct)\n",
    "Inference Latency (single sample, median): 7.85 ms\n",
    "Inference Latency (single sample, 95th percentile): 8.14 ms\n",
    "Inference Latency (single sample, 99th percentile): 8.26 ms\n",
    "Inference Throughput (single sample): 126.58 FPS\n",
    "Batch Throughput: 739.48 FPS\n",
    "\n",
    "On Intel\n",
    "\n",
    "Execution provider: ['CPUExecutionProvider']\n",
    "Accuracy: 89.87% (3008/3347 correct)\n",
    "Inference Latency (single sample, median): 2.51 ms\n",
    "Inference Latency (single sample, 95th percentile): 2.60 ms\n",
    "Inference Latency (single sample, 99th percentile): 2.71 ms\n",
    "Inference Throughput (single sample): 396.18 FPS\n",
    "Batch Throughput: 2057.18 FPS\n",
    "\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try a more conservative approach to static quantization next - we’ll allow an accuracy loss only up to **0.01**.\n",
    "\n",
    "This time, we will see that the quantizer tries a few different “recipes” - in many of them, only some of the operations are quantized, in order to try and reach the target accuracy. After each tuning attempt, it tests the quantized model on the evaluation dataset, to see if it meets the accuracy criterion; if not, it tries again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ONNX model into Intel Neural Compressor\n",
    "model_path = \"models/food11.onnx\"\n",
    "fp32_model = neural_compressor.model.onnx_model.ONNXModel(model_path)\n",
    "\n",
    "# Configure the quantizer\n",
    "config_ptq = neural_compressor.PostTrainingQuantConfig(\n",
    "    accuracy_criterion = neural_compressor.config.AccuracyCriterion(\n",
    "        criterion=\"absolute\",  \n",
    "        tolerable_loss=0.01  # We will tolerate up to 0.01 less accuracy in the quantized model\n",
    "    ),\n",
    "    approach=\"static\", \n",
    "    device='cpu', \n",
    "    quant_level=0,  # 0 is a less aggressive quantization level\n",
    "    quant_format=\"QOperator\", \n",
    "    recipes={\"graph_optimization_level\": \"ENABLE_EXTENDED\"}, \n",
    "    calibration_sampling_size=128\n",
    ")\n",
    "\n",
    "# Find the best quantized model meeting the accuracy criterion\n",
    "q_model = quantization.fit(\n",
    "    model=fp32_model, \n",
    "    conf=config_ptq, \n",
    "    calib_dataloader=eval_dataloader,\n",
    "    eval_dataloader=eval_dataloader, \n",
    "    eval_metric=neural_compressor.metric.Metric(name='topk')\n",
    ")\n",
    "\n",
    "# Save quantized model\n",
    "q_model.save_model_to_file(\"models/food11_quantized_conservative.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the `food11_quantized_conservative.onnx` model from inside the `models` directory.\n",
    "\n",
    "To see the effect of the quantization, we can visualize the models using [Netron](https://netron.app/). Upload the `food11_quantized_conservative.onnx` and see what has changed in the quantized graph, relative to the “aggressive quantization” graph.\n",
    "\n",
    "In this graph, since only some operations are quantized, we have a “Quantize” node before each quantized operation in the graph, and a “Dequantize” node after."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s get the size of the quantized model on disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = \"models/food11_quantized_conservative.onnx\"\n",
    "model_size = os.path.getsize(onnx_model_path) \n",
    "print(f\"Model Size on Disk: {model_size/ (1e6) :.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, evaluate the quantized model. While we see some savings in model size relative to the unquantized model, the additional quantize and dequantize operations can make the inference time much slower.\n",
    "\n",
    "However, these tradeoffs vary from one model to the next, and across implementations and hardware. In some cases, the quantize-dequantize model may still have faster inference times than the unquantized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = \"models/food11_quantized_conservative.onnx\"\n",
    "ort_session = ort.InferenceSession(onnx_model_path, providers=['CPUExecutionProvider'])\n",
    "benchmark_session(ort_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "\n",
    "on AMD EPYC\n",
    "\n",
    "Model Size on Disk: 6.01 MB\n",
    "Accuracy: 90.20% (3019/3347 correct)\n",
    "Inference Latency (single sample, median): 10.20 ms\n",
    "Inference Latency (single sample, 95th percentile): 10.39 ms\n",
    "Inference Latency (single sample, 99th percentile): 10.66 ms\n",
    "Inference Throughput (single sample): 97.87 FPS\n",
    "Batch Throughput: 277.23 FPS\n",
    "\n",
    "On intel\n",
    "\n",
    "Execution provider: ['CPUExecutionProvider']\n",
    "Accuracy: 90.44% (3027/3347 correct)\n",
    "Inference Latency (single sample, median): 6.60 ms\n",
    "Inference Latency (single sample, 95th percentile): 6.66 ms\n",
    "Inference Latency (single sample, 99th percentile): 6.68 ms\n",
    "Inference Throughput (single sample): 151.36 FPS\n",
    "Batch Throughput: 540.19 FPS\n",
    "\n",
    "-->\n",
    "<!--\n",
    "\n",
    "\n",
    "::: {.cell .markdown}\n",
    "\n",
    "### Quantization aware training\n",
    "\n",
    "To achieve the best of both worlds - high accuracy, but the small model size and faster inference time of a quantized model - we can try quantization aware training. In QAT, the effect of quantization is \"simulated\" during training, so that we learn weights that are more robust to quantization. Then, when we quantize the model, we can achieve better accuracy.\n",
    "\n",
    ":::\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are done, download the fully executed notebook from the Jupyter container environment for later reference. (Note: because it is an executable file, and you are downloading it from a site that is not secured with HTTPS, you may have to explicitly confirm the download in some browsers.)\n",
    "\n",
    "Also download the models from inside the `models` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0137fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ✅ Project-specific ONNX optimization for SSEPT\n",
    "\n",
    "from onnxruntime_tools import optimizer\n",
    "from onnxruntime_tools.transformers.onnx_model_bert import BertOptimizationOptions\n",
    "\n",
    "input_model_path = \"/home/cc/data/shared/model/ssept_model.onnx\"\n",
    "output_model_path = \"/home/cc/data/shared/model/ssept_model_optimized.onnx\"\n",
    "\n",
    "opt_options = BertOptimizationOptions('bert')\n",
    "opt_options.enable_embed_layer_norm = True  # Enable optimizations common in transformers\n",
    "\n",
    "optimized_model = optimizer.optimize_model(\n",
    "    input_model_path,\n",
    "    model_type='bert',\n",
    "    optimization_options=opt_options\n",
    ")\n",
    "\n",
    "optimized_model.save_model_to_file(output_model_path)\n",
    "print(f\"✅ Optimized model saved to: {output_model_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
