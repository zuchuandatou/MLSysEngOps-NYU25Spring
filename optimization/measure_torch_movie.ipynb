{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure inference performance of PyTorch model on CPU\n",
    "\n",
    "First, measure the inference performance of model on CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchinfo import summary\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load model in evaluation mode, and print a summary of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"models/SSE_PT10kemb.pth\"  \n",
    "device = torch.device(\"cpu\")\n",
    "model = torch.load(model_path, map_location=device, weights_only=False)\n",
    "model.eval()  \n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and also prepare our test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data_dir = os.getenv(\"MOVIELENS_DATA_DIR\", \"Enhanced-MovieLens\")\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "test_dataset = datasets.ImageFolder(root=os.path.join(movie_data_dir, 'evaluation'), transform=val_test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will measure:\n",
    "\n",
    "-   the size of the model on disk\n",
    "-   the latency when doing inference on single samples\n",
    "-   the throughput when doing inference on batches of data\n",
    "-   and the test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model size\n",
    "\n",
    "We’ll start with model size. Our default `food11.pth` is a finetuned MobileNetV2, which is a small model designed for deployment on edge devices, so it is fairly small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = os.path.getsize(model_path) \n",
    "print(f\"Model Size on Disk: {model_size/ (1e6) :.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test accuracy\n",
    "\n",
    "Next, we’ll measure the accuracy of this model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)  # Get predicted class index\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = (correct / total) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {accuracy:.2f}% ({correct}/{total} correct)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference latency\n",
    "\n",
    "Now, we’ll measure how long it takes the model to return a prediction for a single sample. We will run 100 trials, and then compute aggregate statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 100  # Number of trials\n",
    "\n",
    "# Get a single sample from the test data\n",
    "\n",
    "single_sample, _ = next(iter(test_loader))  \n",
    "single_sample = single_sample[0].unsqueeze(0)  \n",
    "\n",
    "# Warm-up run \n",
    "with torch.no_grad():\n",
    "    model(single_sample)\n",
    "\n",
    "latencies = []\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_trials):\n",
    "        start_time = time.time()\n",
    "        _ = model(single_sample)\n",
    "        latencies.append(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Inference Latency (single sample, median): {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (single sample, 95th percentile): {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (single sample, 99th percentile): {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "print(f\"Inference Throughput (single sample): {num_trials/np.sum(latencies):.2f} FPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch throughput\n",
    "\n",
    "Finally, we’ll measure the rate at which the model can return predictions for batches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 50  # Number of trials\n",
    "\n",
    "# Get a batch from the test data\n",
    "batch_input, _ = next(iter(test_loader))  \n",
    "\n",
    "# Warm-up run \n",
    "with torch.no_grad():\n",
    "    model(batch_input)\n",
    "\n",
    "batch_times = []\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_batches):\n",
    "        start_time = time.time()\n",
    "        _ = model(batch_input)\n",
    "        batch_times.append(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_fps = (batch_input.shape[0] * num_batches) / np.sum(batch_times) \n",
    "print(f\"Batch Throughput: {batch_fps:.2f} FPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are done, download the fully executed notebook from the Jupyter container environment for later reference. (Note: because it is an executable file, and you are downloading it from a site that is not secured with HTTPS, you may have to explicitly confirm the download in some browsers.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model Size on Disk: {model_size/ (1e6) :.2f} MB\")\n",
    "print(f\"Accuracy: {accuracy:.2f}% ({correct}/{total} correct)\")\n",
    "print(f\"Inference Latency (single sample, median): {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (single sample, 95th percentile): {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (single sample, 99th percentile): {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "print(f\"Inference Throughput (single sample): {num_trials/np.sum(latencies):.2f} FPS\")\n",
    "print(f\"Batch Throughput: {batch_fps:.2f} FPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are done, download the fully executed notebook from the Jupyter container environment for later reference. (Note: because it is an executable file, and you are downloading it from a site that is not secured with HTTPS, you may have to explicitly confirm the download in some browsers.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eager mode execution vs compiled model\n",
    "\n",
    "We had just evaluated a model in eager mode. However, in some (although, not all) cases we may get better performance from compiling the model into a graph, and executing it as a graph.\n",
    "\n",
    "Go back to the cell where the model is loaded, and add\n",
    "\n",
    "``` python\n",
    "model.compile()\n",
    "```\n",
    "\n",
    "just below the call to `torch.load`. Then, run the notebook again (“Run \\> Run All Cells”).\n",
    "\n",
    "When you are done, download the fully executed notebook from the Jupyter container environment for later reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- \n",
    "\n",
    "compute_gigaio \n",
    "\n",
    "  Model name:             AMD EPYC 7763 64-Core Processor\n",
    "    CPU family:           25\n",
    "    Model:                1\n",
    "    Thread(s) per core:   2\n",
    "    Core(s) per socket:   64\n",
    "\n",
    "-->\n",
    "<!-- summary for mobilenet model\n",
    "\n",
    "Model Size on Disk: 9.23 MB\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 60.16 ms\n",
    "Inference Latency (single sample, 95th percentile): 77.22 ms\n",
    "Inference Latency (single sample, 99th percentile): 77.37 ms\n",
    "Inference Throughput (single sample): 15.82 FPS\n",
    "Batch Throughput: 83.66 FPS\n",
    "\n",
    "\n",
    "Model Size on Disk: 9.23 MB\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 73.97 ms\n",
    "Inference Latency (single sample, 95th percentile): 83.16 ms\n",
    "Inference Latency (single sample, 99th percentile): 83.94 ms\n",
    "Inference Throughput (single sample): 13.34 FPS\n",
    "Batch Throughput: 98.80 FPS\n",
    "\n",
    "-->\n",
    "<!-- summary for mobilenet compiled model\n",
    "\n",
    "Model Size on Disk: 9.23 MB\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 26.92 ms\n",
    "Inference Latency (single sample, 95th percentile): 49.79 ms\n",
    "Inference Latency (single sample, 99th percentile): 64.55 ms\n",
    "Inference Throughput (single sample): 32.35 FPS\n",
    "Batch Throughput: 249.08 FPS\n",
    "\n",
    "Model Size on Disk: 9.23 MB\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 34.14 ms\n",
    "Inference Latency (single sample, 95th percentile): 53.85 ms\n",
    "Inference Latency (single sample, 99th percentile): 60.23 ms\n",
    "Inference Throughput (single sample): 27.39 FPS\n",
    "Batch Throughput: 281.65 FPS\n",
    "\n",
    "-->\n",
    "<!-- \n",
    "\n",
    "(Intel CPU)\n",
    "\n",
    "Model Size on Disk: 9.23 MB\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 12.69 ms\n",
    "Inference Latency (single sample, 95th percentile): 12.83 ms\n",
    "Inference Latency (single sample, 99th percentile): 12.97 ms\n",
    "Inference Throughput (single sample): 78.73 FPS\n",
    "Batch Throughput: 161.27 FPS\n",
    "\n",
    "With compiling\n",
    "\n",
    "Model Size on Disk: 9.23 MB\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 8.47 ms\n",
    "Inference Latency (single sample, 95th percentile): 8.58 ms\n",
    "Inference Latency (single sample, 99th percentile): 8.79 ms\n",
    "Inference Throughput (single sample): 117.86 FPS\n",
    "Batch Throughput: 474.67 FPS\n",
    "\n",
    "\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e3439b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ✅ Project-specific modification for SSE-PT movie recommendation model\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "from model import SSEPTModel  # your SSE-PT model class\n",
    "from data_loader import get_sample_input  # your input construction utility\n",
    "\n",
    "# Load model\n",
    "model_path = \"/home/cc/data/shared/model/ssept_model.pt\"\n",
    "model = SSEPTModel()\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))\n",
    "model.eval()\n",
    "\n",
    "# Prepare dummy input (adapt to your model input structure)\n",
    "input_tensor = get_sample_input(batch_size=1)\n",
    "\n",
    "# Measure inference time\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    for _ in range(100):\n",
    "        output = model(input_tensor)\n",
    "    end = time.time()\n",
    "\n",
    "avg_latency = (end - start) / 100\n",
    "print(f\"✅ Average SSEPT inference time (CPU): {avg_latency:.6f} sec/sample\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
