{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try a different execution provider\n",
    "\n",
    "Once a model is in ONNX format, we can use it with many *execution providers*. In ONNX, an execution provider an interface that lets ONNX models run with special hardware-specific capabilities. Until now, we have been using the `CPUExecutionProvider`, but if we use hardware-specific capabilities, e.g. switch out generic implementations of graph operations for implementations that are optimized for specific hardware, we can execute exactly the same model, much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test dataset\n",
    "food_11_data_dir = os.getenv(\"FOOD11_DATA_DIR\", \"Food-11\")\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "test_dataset = datasets.ImageFolder(root=os.path.join(food_11_data_dir, 'evaluation'), transform=val_test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_session(ort_session):\n",
    "\n",
    "    print(f\"Execution provider: {ort_session.get_providers()}\")\n",
    "\n",
    "    ## Benchmark accuracy\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images_np = images.numpy()\n",
    "        outputs = ort_session.run(None, {ort_session.get_inputs()[0].name: images_np})[0]\n",
    "        predicted = np.argmax(outputs, axis=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.numpy()).sum()\n",
    "    accuracy = (correct / total) * 100\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.2f}% ({correct}/{total} correct)\")\n",
    "\n",
    "    ## Benchmark inference latency for single sample\n",
    "\n",
    "    num_trials = 100  # Number of trials\n",
    "\n",
    "    # Get a single sample from the test data\n",
    "\n",
    "    single_sample, _ = next(iter(test_loader))  \n",
    "    single_sample = single_sample[:1].numpy()\n",
    "\n",
    "    # Warm-up run\n",
    "    ort_session.run(None, {ort_session.get_inputs()[0].name: single_sample})\n",
    "\n",
    "    latencies = []\n",
    "    for _ in range(num_trials):\n",
    "        start_time = time.time()\n",
    "        ort_session.run(None, {ort_session.get_inputs()[0].name: single_sample})\n",
    "        latencies.append(time.time() - start_time)\n",
    "\n",
    "    print(f\"Inference Latency (single sample, median): {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "    print(f\"Inference Latency (single sample, 95th percentile): {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "    print(f\"Inference Latency (single sample, 99th percentile): {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "    print(f\"Inference Throughput (single sample): {num_trials/np.sum(latencies):.2f} FPS\")\n",
    "\n",
    "    ## Benchmark batch throughput\n",
    "\n",
    "    num_batches = 50  # Number of trials\n",
    "\n",
    "    # Get a batch from the test data\n",
    "    batch_input, _ = next(iter(test_loader))  \n",
    "    batch_input = batch_input.numpy()\n",
    "\n",
    "    # Warm-up run\n",
    "    ort_session.run(None, {ort_session.get_inputs()[0].name: batch_input})\n",
    "\n",
    "    batch_times = []\n",
    "    for _ in range(num_batches):\n",
    "        start_time = time.time()\n",
    "        ort_session.run(None, {ort_session.get_inputs()[0].name: batch_input})\n",
    "        batch_times.append(time.time() - start_time)\n",
    "\n",
    "    batch_fps = (batch_input.shape[0] * num_batches) / np.sum(batch_times) \n",
    "    print(f\"Batch Throughput: {batch_fps:.2f} FPS\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CPU execution provider\n",
    "\n",
    "First, for reference, we’ll repeat our performance test for the (unquantized model with) `CPUExecutionProvider`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = \"models/food11.onnx\"\n",
    "ort_session = ort.InferenceSession(onnx_model_path, providers=['CPUExecutionProvider'])\n",
    "benchmark_session(ort_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "Execution provider: ['CPUExecutionProvider']\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 9.93 ms\n",
    "Inference Latency (single sample, 95th percentile): 14.20 ms\n",
    "Inference Latency (single sample, 99th percentile): 14.43 ms\n",
    "Inference Throughput (single sample): 91.10 FPS\n",
    "Batch Throughput: 1042.47 FPS\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CUDA execution provider\n",
    "\n",
    "Next, we’ll try it with the CUDA execution provider, which will execute the model on the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = \"models/food11.onnx\"\n",
    "ort_session = ort.InferenceSession(onnx_model_path, providers=['CUDAExecutionProvider'])\n",
    "benchmark_session(ort_session)\n",
    "ort.get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "Execution provider: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 0.89 ms\n",
    "Inference Latency (single sample, 95th percentile): 0.90 ms\n",
    "Inference Latency (single sample, 99th percentile): 0.91 ms\n",
    "Inference Throughput (single sample): 1117.06 FPS\n",
    "Batch Throughput: 5181.99 FPS\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorRT execution provider\n",
    "\n",
    "The TensorRT execution provider will optimize the model for inference on NVIDIA GPUs. It will take a long time to run this cell, because it spends a lot of time optimizing the model (finding the best subgraphs, etc.) - but once the model is loaded, its inference time will be much faster than any of our previous tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = \"models/food11.onnx\"\n",
    "ort_session = ort.InferenceSession(onnx_model_path, providers=['TensorrtExecutionProvider'])\n",
    "benchmark_session(ort_session)\n",
    "ort.get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "Execution provider: ['TensorrtExecutionProvider', 'CPUExecutionProvider']\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 0.63 ms\n",
    "Inference Latency (single sample, 95th percentile): 0.64 ms\n",
    "Inference Latency (single sample, 99th percentile): 0.70 ms\n",
    "Inference Throughput (single sample): 1572.61 FPS\n",
    "Batch Throughput: 9274.45 FPS\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenVINO execution provider\n",
    "\n",
    "Even just on CPU, we can still use an optimized execution provider to improve inference performance. We will try out the Intel [OpenVINO](https://github.com/openvinotoolkit/openvino) execution provider. However, ONNX runtime can be built to support CUDA/TensorRT or OpenVINO, but not both at the same time, so we will need to bring up a new container.\n",
    "\n",
    "Close this Jupyter server tab - you will reopen it shortly, with a new token.\n",
    "\n",
    "Go back to your SSH session on “node-serve-model”, and build a container image for a Jupyter server with ONNX and OpenVINO:\n",
    "\n",
    "``` bash\n",
    "# run on node-serve-model \n",
    "docker build -t jupyter-onnx-openvino -f serve-model-chi/docker/Dockerfile.jupyter-onnx-cpu .\n",
    "```\n",
    "\n",
    "Stop the current Jupyter server:\n",
    "\n",
    "``` bash\n",
    "# run on node-serve-model \n",
    "docker stop jupyter\n",
    "```\n",
    "\n",
    "Then, launch a container with the new image you just built:\n",
    "\n",
    "``` bash\n",
    "# run on node-serve-model \n",
    "docker run  -d --rm  -p 8888:8888 \\\n",
    "    --shm-size 16G \\\n",
    "    -v ~/serve-model-chi/workspace:/home/jovyan/work/ \\\n",
    "    -v food11:/mnt/ \\\n",
    "    -e FOOD11_DATA_DIR=/mnt/Food-11 \\\n",
    "    --name jupyter \\\n",
    "    jupyter-onnx-openvino\n",
    "```\n",
    "\n",
    "Run\n",
    "\n",
    "``` bash\n",
    "# run on node-serve-model \n",
    "docker logs jupyter\n",
    "```\n",
    "\n",
    "and look for a line like\n",
    "\n",
    "    http://127.0.0.1:8888/lab?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "\n",
    "Paste this into a browser tab, but in place of 127.0.0.1, substitute the floating IP assigned to your instance, to open the Jupyter notebook interface that is running *on your compute instance*.\n",
    "\n",
    "Then, in the file browser on the left side, open the “work” directory and then click on the `7_ep_onnx.ipynb` notebook to continue.\n",
    "\n",
    "Run the three cells at the top, which `import` libraries, set up the data loaders, and define the `benchmark_session` function. Then, skip to the OpenVINO section and run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = \"models/food11.onnx\"\n",
    "ort_session = ort.InferenceSession(onnx_model_path, providers=['OpenVINOExecutionProvider'])\n",
    "benchmark_session(ort_session)\n",
    "ort.get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "\n",
    "On AMD EPYC\n",
    "\n",
    "Execution provider: ['OpenVINOExecutionProvider', 'CPUExecutionProvider']\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 1.39 ms\n",
    "Inference Latency (single sample, 95th percentile): 1.89 ms\n",
    "Inference Latency (single sample, 99th percentile): 1.92 ms\n",
    "Inference Throughput (single sample): 646.63 FPS\n",
    "Batch Throughput: 1624.30 FPS\n",
    "\n",
    "On Intel\n",
    "\n",
    "Execution provider: ['OpenVINOExecutionProvider', 'CPUExecutionProvider']\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 1.55 ms\n",
    "Inference Latency (single sample, 95th percentile): 1.76 ms\n",
    "Inference Latency (single sample, 99th percentile): 1.81 ms\n",
    "Inference Throughput (single sample): 663.72 FPS\n",
    "Batch Throughput: 2453.48 FPS\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are done, download the fully executed notebook from the Jupyter container environment for later reference. (Note: because it is an executable file, and you are downloading it from a site that is not secured with HTTPS, you may have to explicitly confirm the download in some browsers.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c06afa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ✅ Benchmark SSEPT ONNX model across different execution providers\n",
    "\n",
    "import onnxruntime as ort\n",
    "import time\n",
    "from data_loader import get_sample_input_numpy  # provide input as dict\n",
    "\n",
    "onnx_model_path = \"/home/cc/data/shared/model/ssept_model.onnx\"\n",
    "input_data = get_sample_input_numpy(batch_size=1)\n",
    "\n",
    "providers = [\"CPUExecutionProvider\"]\n",
    "if \"CUDAExecutionProvider\" in ort.get_available_providers():\n",
    "    providers.insert(0, \"CUDAExecutionProvider\")\n",
    "\n",
    "for provider in providers:\n",
    "    print(f\"🔍 Testing on {provider}\")\n",
    "    session = ort.InferenceSession(onnx_model_path, providers=[provider])\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        session.run(None, input_data)\n",
    "\n",
    "    # Timed inference\n",
    "    start = time.time()\n",
    "    for _ in range(100):\n",
    "        session.run(None, input_data)\n",
    "    end = time.time()\n",
    "\n",
    "    avg_latency = (end - start) / 100\n",
    "    print(f\"✅ {provider} avg latency: {avg_latency:.6f} sec/sample\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
