{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure inference performance of ONNX model on CPU\n",
    "\n",
    "To squeeze even more inference performance out of our model, we are going to convert it to ONNX format, which allows models from different frameworks (PyTorch, Tensorflow, Keras), to be deployed on a variety of different hardware platforms (CPU, GPU, edge devices), using many optimizations (graph optimizations, quantization, target device-specific implementations, and more).\n",
    "\n",
    "After finishing this section, you should know:\n",
    "\n",
    "-   how to convert a PyTorch model to ONNX\n",
    "-   how to measure the inference latency and batch throughput of the ONNX model\n",
    "\n",
    "and then you will use it to evaluate the optimized models you develop in the next section.\n",
    "\n",
    "You will execute this notebook *in a Jupyter container running on a compute instance*, not on the general-purpose Chameleon Jupyter environment from which you provision resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e05952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from data_utils import build_sample_batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"model/SSE_PT10kemb.pth\"\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = SSEPTModel()\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "onnx_model_path = \"/mnt/models/ssept_dynamic.onnx\"\n",
    "\n",
    "dummy_input = (\n",
    "    torch.tensor([[1]], dtype=torch.long),                 # userId\n",
    "    torch.tensor([[101]], dtype=torch.long),               # movieId\n",
    "    torch.randint(0, 1000, (1, 10), dtype=torch.long),      # cast\n",
    "    torch.randint(0, 20, (1, 5), dtype=torch.long),         # genre\n",
    "    torch.randn(1, 768),                                   # transcript_embedding\n",
    "    torch.randn(1, 512)                                    # audio_embedding\n",
    ")\n",
    "\n",
    "# 导出为 ONNX 模型\n",
    "torch.onnx.export(model, dummy_input, onnx_model_path,\n",
    "                  export_params=True,\n",
    "                  opset_version=20,\n",
    "                  do_constant_folding=True,\n",
    "                  input_names=['userId', 'movieId', 'cast', 'genre', 'transcript_embedding', 'audio_embedding'],\n",
    "                  output_names=['output'],\n",
    "                  dynamic_axes={'userId': {0: 'batch_size'},\n",
    "                                'movieId': {0: 'batch_size'},\n",
    "                                'cast': {0: 'batch_size'},\n",
    "                                'genre': {0: 'batch_size'},\n",
    "                                'transcript_embedding': {0: 'batch_size'},\n",
    "                                'audio_embedding': {0: 'batch_size'},\n",
    "                                'output': {0: 'batch_size'}})\n",
    "\n",
    "print(f\"ONNX model saved to: {onnx_model_path}\")\n",
    "\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print(\"ONNX model structure is valid.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an inference session\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6759a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型路径\n",
    "model_path =\"/model/SSE_PT10kemb.pth\"\n",
    "onnx_model_path = \"/mnt/movielens/models/ssept_dynamic.onnx\"\n",
    "\n",
    "# 加载模型\n",
    "device = torch.device(\"cpu\")\n",
    "model = SSEPTModel()\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造 dummy 输入并导出为 ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = build_sample_batch(batch_size=1)  # 返回 Dict[str, Tensor]\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (dummy_input,),\n",
    "    onnx_model_path,\n",
    "    export_params=True,\n",
    "    opset_version=20,\n",
    "    do_constant_folding=True,\n",
    "    input_names=list(dummy_input.keys()),\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={k: {0: \"batch_size\"} for k in dummy_input.keys()}\n",
    ")\n",
    "print(f\"ONNX model saved to {onnx_model_path}\")\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "onnx.checker.check_model(onnx_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建 ONNX Inference Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session = ort.InferenceSession(onnx_model_path, providers=[\"CPUExecutionProvider\"])\n",
    "print(\"Execution Providers:\", ort_session.get_providers())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_batch(batch_size=32):\n",
    "    return build_sample_batch(batch_size=batch_size)\n",
    "\n",
    "test_loader = [get_test_batch(32) for _ in range(10)] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684e6a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for inputs in test_loader:\n",
    "    inputs_numpy = {k: v.numpy() for k, v in inputs.items()}\n",
    "    outputs = ort_session.run(None, inputs_numpy)[0]\n",
    "    predicted = np.argmax(outputs, axis=1)\n",
    "    labels = inputs[\"movie_id\"].numpy().flatten()\n",
    "    total += labels.shape[0]\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "accuracy = (correct / total) * 100\n",
    "print(f\"Accuracy: {accuracy:.2f}% ({correct}/{total} correct)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model size\n",
    "\n",
    "We are also concerned with the size of the ONNX model on disk. It will be similar to the equivalent PyTorch model size (to start!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb30f0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = os.path.getsize(onnx_model_path) \n",
    "print(f\"Model Size on Disk: {model_size/ (1e6) :.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference latency\n",
    "\n",
    "Now, we’ll measure how long it takes the model to return a prediction for a single sample. We will run 100 trials, and then compute aggregate statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4768d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 100  # Number of trials\n",
    "\n",
    "# Get a single sample from the test data\n",
    "\n",
    "single_sample, _ = next(iter(test_loader))  \n",
    "single_sample = single_sample[:1].numpy()\n",
    "\n",
    "ort_session.run(None, {ort_session.get_inputs()[0].name: single_sample})\n",
    "\n",
    "latencies = []\n",
    "for _ in range(num_trials):\n",
    "    start_time = time.time()\n",
    "    ort_session.run(None, {ort_session.get_inputs()[0].name: single_sample})\n",
    "    latencies.append(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75b6fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Inference Latency (single sample, median): {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (single sample, 95th percentile): {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (single sample, 99th percentile): {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "print(f\"Inference Throughput (single sample): {num_trials/np.sum(latencies):.2f} FPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch throughput\n",
    "\n",
    "Finally, we’ll measure the rate at which the model can return predictions for batches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9df394",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 50  \n",
    "\n",
    "# Get a batch from the test data\n",
    "batch_input_np = {k: v.numpy() for k, v in batch_input.items()}\n",
    "\n",
    "ort_session.run(None, {ort_session.get_inputs()[0].name: batch_input})\n",
    "\n",
    "batch_times = []\n",
    "for _ in range(num_batches):\n",
    "    start_time = time.time()\n",
    "    ort_session.run(None, {ort_session.get_inputs()[0].name: batch_input})\n",
    "    batch_times.append(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3da21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_fps = (batch_input.shape[0] * num_batches) / np.sum(batch_times) \n",
    "print(f\"Batch Throughput: {batch_fps:.2f} FPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5311cd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {accuracy:.2f}% ({correct}/{total} correct)\")\n",
    "print(f\"Model Size on Disk: {model_size/ (1e6) :.2f} MB\")\n",
    "print(f\"Inference Latency (single sample, median): {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (single sample, 95th percentile): {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (single sample, 99th percentile): {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "print(f\"Inference Throughput (single sample): {num_trials/np.sum(latencies):.2f} FPS\")\n",
    "print(f\"Batch Throughput: {batch_fps:.2f} FPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- summary for mobilenet\n",
    "\n",
    "Model Size on Disk: 8.92 MB\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 8.92 ms\n",
    "Inference Latency (single sample, 95th percentile): 9.15 ms\n",
    "Inference Latency (single sample, 99th percentile): 9.41 ms\n",
    "Inference Throughput (single sample): 112.06 FPS\n",
    "Batch Throughput: 993.48 FPS\n",
    "\n",
    "Model Size on Disk: 8.92 MB\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 9.64 ms\n",
    "Inference Latency (single sample, 95th percentile): 10.57 ms\n",
    "Inference Latency (single sample, 99th percentile): 11.72 ms\n",
    "Inference Latency (single sample, std error): 0.04 ms\n",
    "Inference Throughput (single sample): 102.52 FPS\n",
    "Batch Throughput: 1083.57 FPS\n",
    "\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Model Size on Disk: 8.92 MB\n",
    "Inference Latency (single sample, median): 16.24 ms\n",
    "Inference Latency (single sample, 95th percentile): 18.06 ms\n",
    "Inference Latency (single sample, 99th percentile): 18.72 ms\n",
    "Inference Throughput (single sample): 63.51 FPS\n",
    "Batch Throughput: 1103.28 FPS\n",
    "\n",
    "\n",
    "-->\n",
    "<!-- summary for mobilenet with graph optimization\n",
    "\n",
    "Model Size on Disk: 8.91 MB\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 9.31 ms\n",
    "Inference Latency (single sample, 95th percentile): 9.47 ms\n",
    "Inference Latency (single sample, 99th percentile): 9.71 ms\n",
    "Inference Throughput (single sample): 107.22 FPS\n",
    "Batch Throughput: 1091.58 FPS\n",
    "\n",
    "Model Size on Disk: 8.91 MB\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 9.95 ms\n",
    "Inference Latency (single sample, 95th percentile): 10.14 ms\n",
    "Inference Latency (single sample, 99th percentile): 10.70 ms\n",
    "Inference Latency (single sample, std error): 0.02 ms\n",
    "Inference Throughput (single sample): 100.18 FPS\n",
    "Batch Throughput: 1022.77 FPS\n",
    "\n",
    "Model Size on Disk: 8.91 MB\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 9.55 ms\n",
    "Inference Latency (single sample, 95th percentile): 10.58 ms\n",
    "Inference Latency (single sample, 99th percentile): 11.14 ms\n",
    "Inference Latency (single sample, std error): 0.04 ms\n",
    "Inference Throughput (single sample): 102.97 FPS\n",
    "Batch Throughput: 1079.81 FPS\n",
    "\n",
    "\n",
    "-->\n",
    "<!-- \n",
    "\n",
    "(Intel CPU)\n",
    "\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Model Size on Disk: 8.92 MB\n",
    "Inference Latency (single sample, median): 4.53 ms\n",
    "Inference Latency (single sample, 95th percentile): 4.63 ms\n",
    "Inference Latency (single sample, 99th percentile): 4.99 ms\n",
    "Inference Throughput (single sample): 218.75 FPS\n",
    "Batch Throughput: 2519.80 FPS\n",
    "\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are done, download the fully executed notebook from the Jupyter container environment for later reference. (Note: because it is an executable file, and you are downloading it from a site that is not secured with HTTPS, you may have to explicitly confirm the download in some browsers.)\n",
    "\n",
    "Also download the `food11.onnx` model from inside the `models` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87780a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SSE-PT ONNX Inference Performance Evaluation for Movie Recommendation\n",
    "\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Load ONNX model\n",
    "onnx_model_path = \"/mnt/models/ssept_dynamic.onnx\"\n",
    "session = ort.InferenceSession(onnx_model_path, providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "# Dummy input for SSEPT \n",
    "input_tensor = {\n",
    "    'user_id': np.array([[1]], dtype=np.int64),\n",
    "    'movie_id': np.array([[101]], dtype=np.int64),\n",
    "    'cast': np.random.randint(0, 1000, size=(1, 10), dtype=np.int64),\n",
    "    'genre': np.random.randint(0, 20, size=(1, 5), dtype=np.int64),\n",
    "    'transcript_embedding': np.random.rand(1, 768).astype(np.float32),\n",
    "    'audio_embedding': np.random.rand(1, 512).astype(np.float32)\n",
    "}\n",
    "\n",
    "for _ in range(10):\n",
    "    _ = session.run(None, input_tensor)\n",
    "\n",
    "# Measure inference latency\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    outputs = session.run(None, input_tensor)\n",
    "end = time.time()\n",
    "\n",
    "avg_latency = (end - start) / 100\n",
    "print(f\"Average inference time for SSEPT ONNX (CPU): {avg_latency:.6f} sec/sample\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
