{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure inference performance of ONNX model on CPU\n",
    "\n",
    "To squeeze even more inference performance out of our model, we are going to convert it to ONNX format, which allows models from different frameworks (PyTorch, Tensorflow, Keras), to be deployed on a variety of different hardware platforms (CPU, GPU, edge devices), using many optimizations (graph optimizations, quantization, target device-specific implementations, and more)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from utilities import build_model_from_ckpt, pad_or_truncate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test dataset\n",
    "SEQ_LEN = 50 \n",
    "class MovieLensTestDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.data = self.df.to_dict(orient=\"records\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data[idx]\n",
    "        user_id = int(row[\"user_id\"])\n",
    "        sequence = eval(row[\"sequence\"]) if isinstance(row[\"sequence\"], str) else row[\"sequence\"]\n",
    "        sequence = pad_or_truncate(sequence, SEQ_LEN)\n",
    "        return user_id, sequence\n",
    "\n",
    "# load data\n",
    "movielens_data_dir = os.getenv(\"MOVIELENS_DATA_DIR\", \"/mnt/data\")\n",
    "test_dataset = MovieLensTestDataset(os.path.join(movielens_data_dir, \"test.csv\"))\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# ONNX model\n",
    "onnx_model_path = \"models/SSE_PT10kemb.onnx\" \n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "ort_session = ort.InferenceSession(onnx_model_path, providers=['CPUExecutionProvider'])\n",
    "\n",
    "inputs = ort_session.get_inputs()\n",
    "user_input_name = inputs[0].name \n",
    "seq_input_name = inputs[1].name "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an inference session\n",
    "\n",
    "To use an ONNX model, we create an *inference session*, and then use the model within that session. \n",
    "Let’s start an inference session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = \"models/SSE_PT10kemb.onnx\" \n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "ort_session = ort.InferenceSession(onnx_model_path, providers=['CPUExecutionProvider'])\n",
    "\n",
    "inputs = ort_session.get_inputs()\n",
    "user_input_name = inputs[0].name \n",
    "seq_input_name = inputs[1].name "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test accuracy\n",
    "\n",
    "First, let’s measure accuracy on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct, total = 0, 0\n",
    "for user_ids, sequences in test_loader:\n",
    "    u = np.array(user_ids)\n",
    "    s = np.stack(sequences)\n",
    "    outputs = ort_session.run(None, {\n",
    "        user_input_name: u,\n",
    "        seq_input_name: s\n",
    "    })[0]\n",
    "    preds = np.argmax(outputs, axis=1)\n",
    "    total += len(preds)\n",
    "    correct += len(preds) \n",
    "\n",
    "accuracy = correct / total * 100\n",
    "print(f\"Accuracy (dummy): {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model size\n",
    "\n",
    "We are also concerned with the size of the ONNX model on disk. It will be similar to the equivalent PyTorch model size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = os.path.getsize(onnx_model_path)\n",
    "print(f\"Model Size on Disk: {model_size / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference latency\n",
    "\n",
    "Now, we’ll measure how long it takes the model to return a prediction for a single sample. We will run 100 trials, and then compute aggregate statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user, seq = test_dataset[0]\n",
    "u = np.array([user])\n",
    "s = np.array([seq])\n",
    "\n",
    "# warm-up\n",
    "ort_session.run(None, {user_input_name: u, seq_input_name: s})\n",
    "\n",
    "latencies = []\n",
    "for _ in range(100):\n",
    "    start = time.time()\n",
    "    ort_session.run(None, {user_input_name: u, seq_input_name: s})\n",
    "    latencies.append(time.time() - start)\n",
    "\n",
    "print(f\"Inference Latency (median): {np.percentile(latencies, 50)*1000:.2f} ms\")\n",
    "print(f\"Inference Latency (95th): {np.percentile(latencies, 95)*1000:.2f} ms\")\n",
    "print(f\"Inference Latency (99th): {np.percentile(latencies, 99)*1000:.2f} ms\")\n",
    "print(f\"Inference Throughput (single sample): {100/np.sum(latencies):.2f} FPS\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch throughput\n",
    "\n",
    "Finally, we’ll measure the rate at which the model can return predictions for batches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_user, batch_seq = next(iter(test_loader))\n",
    "u = np.array(batch_user)\n",
    "s = np.stack(batch_seq)\n",
    "\n",
    "# warm-up\n",
    "ort_session.run(None, {user_input_name: u, seq_input_name: s})\n",
    "\n",
    "batch_times = []\n",
    "for _ in range(50):\n",
    "    start = time.time()\n",
    "    ort_session.run(None, {user_input_name: u, seq_input_name: s})\n",
    "    batch_times.append(time.time() - start)\n",
    "\n",
    "batch_fps = (len(batch_user) * 50) / np.sum(batch_times)\n",
    "print(f\"Batch Throughput: {batch_fps:.2f} FPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model Size on Disk: {model_size / 1e6:.2f} MB\")\n",
    "print(f\"Accuracy: {accuracy:.2f}% ({correct}/{total} dummy)\")\n",
    "print(f\"Inference Latency (single sample, median): {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (single sample, 95th percentile): {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (single sample, 99th percentile): {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "print(f\"Inference Throughput (single sample): {100/np.sum(latencies):.2f} FPS\")\n",
    "print(f\"Batch Throughput: {batch_fps:.2f} FPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- summary for mobilenet\n",
    "\n",
    "Model Size on Disk: 8.92 MB\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 8.92 ms\n",
    "Inference Latency (single sample, 95th percentile): 9.15 ms\n",
    "Inference Latency (single sample, 99th percentile): 9.41 ms\n",
    "Inference Throughput (single sample): 112.06 FPS\n",
    "Batch Throughput: 993.48 FPS\n",
    "\n",
    "Model Size on Disk: 8.92 MB\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 9.64 ms\n",
    "Inference Latency (single sample, 95th percentile): 10.57 ms\n",
    "Inference Latency (single sample, 99th percentile): 11.72 ms\n",
    "Inference Latency (single sample, std error): 0.04 ms\n",
    "Inference Throughput (single sample): 102.52 FPS\n",
    "Batch Throughput: 1083.57 FPS\n",
    "\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Model Size on Disk: 8.92 MB\n",
    "Inference Latency (single sample, median): 16.24 ms\n",
    "Inference Latency (single sample, 95th percentile): 18.06 ms\n",
    "Inference Latency (single sample, 99th percentile): 18.72 ms\n",
    "Inference Throughput (single sample): 63.51 FPS\n",
    "Batch Throughput: 1103.28 FPS\n",
    "\n",
    "\n",
    "-->\n",
    "<!-- summary for mobilenet with graph optimization\n",
    "\n",
    "Model Size on Disk: 8.91 MB\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 9.31 ms\n",
    "Inference Latency (single sample, 95th percentile): 9.47 ms\n",
    "Inference Latency (single sample, 99th percentile): 9.71 ms\n",
    "Inference Throughput (single sample): 107.22 FPS\n",
    "Batch Throughput: 1091.58 FPS\n",
    "\n",
    "Model Size on Disk: 8.91 MB\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 9.95 ms\n",
    "Inference Latency (single sample, 95th percentile): 10.14 ms\n",
    "Inference Latency (single sample, 99th percentile): 10.70 ms\n",
    "Inference Latency (single sample, std error): 0.02 ms\n",
    "Inference Throughput (single sample): 100.18 FPS\n",
    "Batch Throughput: 1022.77 FPS\n",
    "\n",
    "Model Size on Disk: 8.91 MB\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 9.55 ms\n",
    "Inference Latency (single sample, 95th percentile): 10.58 ms\n",
    "Inference Latency (single sample, 99th percentile): 11.14 ms\n",
    "Inference Latency (single sample, std error): 0.04 ms\n",
    "Inference Throughput (single sample): 102.97 FPS\n",
    "Batch Throughput: 1079.81 FPS\n",
    "\n",
    "\n",
    "-->\n",
    "<!-- \n",
    "\n",
    "(Intel CPU)\n",
    "\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Model Size on Disk: 8.92 MB\n",
    "Inference Latency (single sample, median): 4.53 ms\n",
    "Inference Latency (single sample, 95th percentile): 4.63 ms\n",
    "Inference Latency (single sample, 99th percentile): 4.99 ms\n",
    "Inference Throughput (single sample): 218.75 FPS\n",
    "Batch Throughput: 2519.80 FPS\n",
    "\n",
    "\n",
    "-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
