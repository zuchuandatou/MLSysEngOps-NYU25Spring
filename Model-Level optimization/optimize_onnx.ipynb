{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply optimizations to ONNX model\n",
    "\n",
    "Now that we have an ONNX model, we can apply some basic optimizations. After completing this section, you should be able to apply:\n",
    "\n",
    "-   graph optimizations, e.g. fusing operations\n",
    "-   post-training quantization (dynamic and static)\n",
    "-   and hardware-specific execution providers\n",
    "\n",
    "to improve inference performance.\n",
    "\n",
    "You will execute this notebook *in a Jupyter container running on a compute instance*, not on the general-purpose Chameleon Jupyter environment from which you provision resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are going to evaluate several models, we’ll define a benchmark function here to help us compare them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1374159 valid user sequences from /mnt/data/evaluation/movielens_192m_eval.txt\n"
     ]
    }
   ],
   "source": [
    "from utilities import pad_or_truncate\n",
    "\n",
    "class SequentialEvalDataset(Dataset):\n",
    "    def __init__(self, filepath, seq_max_len=100):\n",
    "        self.user_sequences = {}\n",
    "        with open(filepath, \"r\") as f:\n",
    "            for line in f:\n",
    "                uid, iid = map(int, line.strip().split(\"\\t\"))\n",
    "                self.user_sequences.setdefault(uid, []).append(iid)\n",
    "\n",
    "        self.samples = []\n",
    "        for uid, seq in self.user_sequences.items():\n",
    "            if len(seq) < 2:\n",
    "                continue\n",
    "            self.samples.append((uid, seq[:-1], seq[-1]))  # label 可选\n",
    "\n",
    "        self.seq_max_len = seq_max_len\n",
    "\n",
    "        print(f\"Loaded {len(self.samples)} valid user sequences from {filepath}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        uid, seq, _ = self.samples[idx]\n",
    "        return (\n",
    "            torch.tensor(uid, dtype=torch.long),\n",
    "            torch.tensor(pad_or_truncate(seq, self.seq_max_len), dtype=torch.long)\n",
    "        )\n",
    "\n",
    "dataset = SequentialEvalDataset(\"/mnt/data/evaluation/movielens_192m_eval.txt\", seq_max_len=100)\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_session(ort_session):\n",
    "    print(f\"Execution provider: {ort_session.get_providers()}\")\n",
    "\n",
    "    user_input_name = ort_session.get_inputs()[0].name\n",
    "    seq_input_name = ort_session.get_inputs()[1].name\n",
    "\n",
    "    # Single sample latency\n",
    "    user_tensor, seq_tensor = dataset[0]\n",
    "    u = user_tensor.unsqueeze(0).numpy()\n",
    "    s = seq_tensor.unsqueeze(0).numpy()\n",
    "\n",
    "    ort_session.run(None, {user_input_name: u, seq_input_name: s})  # warmup\n",
    "\n",
    "    latencies = []\n",
    "    for _ in range(100):\n",
    "        start = time.time()\n",
    "        ort_session.run(None, {user_input_name: u, seq_input_name: s})\n",
    "        latencies.append(time.time() - start)\n",
    "\n",
    "    print(f\"Inference Latency (median): {np.percentile(latencies, 50)*1000:.2f} ms\")\n",
    "    print(f\"Inference Latency (95th): {np.percentile(latencies, 95)*1000:.2f} ms\")\n",
    "    print(f\"Inference Latency (99th): {np.percentile(latencies, 99)*1000:.2f} ms\")\n",
    "    print(f\"Inference Throughput (single sample): {100/np.sum(latencies):.2f} FPS\")\n",
    "\n",
    "    # Batch throughput\n",
    "    user_tensor, seq_tensor = next(iter(loader))\n",
    "    u = user_tensor.numpy()\n",
    "    s = seq_tensor.numpy()\n",
    "\n",
    "    ort_session.run(None, {user_input_name: u, seq_input_name: s})  # warmup\n",
    "\n",
    "    batch_times = []\n",
    "    for _ in range(50):\n",
    "        start = time.time()\n",
    "        ort_session.run(None, {user_input_name: u, seq_input_name: s})\n",
    "        batch_times.append(time.time() - start)\n",
    "\n",
    "    batch_fps = (len(user_tensor) * 50) / np.sum(batch_times)\n",
    "    print(f\"Batch Throughput: {batch_fps:.2f} FPS\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply basic graph optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized ONNX model saved to models/SSE_PT10kemb_optimized.onnx\n"
     ]
    }
   ],
   "source": [
    "onnx_model_path = \"models/SSE_PT10kemb.onnx\"\n",
    "optimized_model_path = \"models/SSE_PT10kemb_optimized.onnx\"\n",
    "\n",
    "session_options = ort.SessionOptions()\n",
    "session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n",
    "session_options.optimized_model_filepath = optimized_model_path\n",
    "\n",
    "ort_session = ort.InferenceSession(\n",
    "    onnx_model_path,\n",
    "    sess_options=session_options,\n",
    "    providers=['CPUExecutionProvider']\n",
    ")\n",
    "\n",
    "print(f\"Optimized ONNX model saved to {optimized_model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, evaluate the optimized model. The graph optimizations may improve the inference performance, may have negligible effect, OR they can make it worse, depending on the model and the hardware environment in which the model is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution provider: ['CPUExecutionProvider']\n",
      "Inference Latency (median): 1.91 ms\n",
      "Inference Latency (95th): 1.93 ms\n",
      "Inference Latency (99th): 2.19 ms\n",
      "Inference Throughput (single sample): 522.37 FPS\n",
      "Batch Throughput: 1072.27 FPS\n"
     ]
    }
   ],
   "source": [
    "optimized_session = ort.InferenceSession(optimized_model_path, providers=['CPUExecutionProvider'])\n",
    "benchmark_session(optimized_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "\n",
    "On gigaio AMD EPYC:\n",
    "\n",
    "\n",
    "Execution provider: ['CPUExecutionProvider']\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 8.70 ms\n",
    "Inference Latency (single sample, 95th percentile): 8.88 ms\n",
    "Inference Latency (single sample, 99th percentile): 9.24 ms\n",
    "Inference Throughput (single sample): 114.63 FPS\n",
    "Batch Throughput: 1153.63 FPS\n",
    "\n",
    "On liqid Intel:\n",
    "\n",
    "Execution provider: ['CPUExecutionProvider']\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 4.63 ms\n",
    "Inference Latency (single sample, 95th percentile): 4.67 ms\n",
    "Inference Latency (single sample, 99th percentile): 4.75 ms\n",
    "Inference Throughput (single sample): 214.45 FPS\n",
    "Batch Throughput: 2488.54 FPS\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply post training quantization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamic quantization\n",
    "\n",
    "We will start with dynamic quantization. No calibration dataset is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 11:51:15 [INFO] Start auto tuning.\n",
      "2025-05-13 11:51:15 [INFO] Quantize model without tuning!\n",
      "2025-05-13 11:51:15 [INFO] Quantize the model with default configuration without evaluating the model.                To perform the tuning process, please either provide an eval_func or provide an                    eval_dataloader an eval_metric.\n",
      "2025-05-13 11:51:15 [INFO] Adaptor has 5 recipes.\n",
      "2025-05-13 11:51:15 [INFO] 0 recipes specified by user.\n",
      "2025-05-13 11:51:15 [INFO] 3 recipes require future tuning.\n",
      "2025-05-13 11:51:15 [INFO] *** Initialize auto tuning\n",
      "2025-05-13 11:51:15 [INFO] {\n",
      "2025-05-13 11:51:15 [INFO]     'PostTrainingQuantConfig': {\n",
      "2025-05-13 11:51:15 [INFO]         'AccuracyCriterion': {\n",
      "2025-05-13 11:51:15 [INFO]             'criterion': 'relative',\n",
      "2025-05-13 11:51:15 [INFO]             'higher_is_better': True,\n",
      "2025-05-13 11:51:15 [INFO]             'tolerable_loss': 0.01,\n",
      "2025-05-13 11:51:15 [INFO]             'absolute': None,\n",
      "2025-05-13 11:51:15 [INFO]             'keys': <bound method AccuracyCriterion.keys of <neural_compressor.config.AccuracyCriterion object at 0x72dfc8759dc0>>,\n",
      "2025-05-13 11:51:15 [INFO]             'relative': 0.01\n",
      "2025-05-13 11:51:15 [INFO]         },\n",
      "2025-05-13 11:51:15 [INFO]         'approach': 'post_training_dynamic_quant',\n",
      "2025-05-13 11:51:15 [INFO]         'backend': 'default',\n",
      "2025-05-13 11:51:15 [INFO]         'calibration_sampling_size': [\n",
      "2025-05-13 11:51:15 [INFO]             100\n",
      "2025-05-13 11:51:15 [INFO]         ],\n",
      "2025-05-13 11:51:15 [INFO]         'device': 'cpu',\n",
      "2025-05-13 11:51:15 [INFO]         'domain': 'auto',\n",
      "2025-05-13 11:51:15 [INFO]         'example_inputs': 'Not printed here due to large size tensors...',\n",
      "2025-05-13 11:51:15 [INFO]         'excluded_precisions': [\n",
      "2025-05-13 11:51:15 [INFO]         ],\n",
      "2025-05-13 11:51:15 [INFO]         'framework': 'onnxruntime',\n",
      "2025-05-13 11:51:15 [INFO]         'inputs': [\n",
      "2025-05-13 11:51:15 [INFO]         ],\n",
      "2025-05-13 11:51:15 [INFO]         'model_name': '',\n",
      "2025-05-13 11:51:15 [INFO]         'op_name_dict': None,\n",
      "2025-05-13 11:51:15 [INFO]         'op_type_dict': None,\n",
      "2025-05-13 11:51:15 [INFO]         'outputs': [\n",
      "2025-05-13 11:51:15 [INFO]         ],\n",
      "2025-05-13 11:51:15 [INFO]         'quant_format': 'default',\n",
      "2025-05-13 11:51:15 [INFO]         'quant_level': 'auto',\n",
      "2025-05-13 11:51:15 [INFO]         'recipes': {\n",
      "2025-05-13 11:51:15 [INFO]             'smooth_quant': False,\n",
      "2025-05-13 11:51:15 [INFO]             'smooth_quant_args': {\n",
      "2025-05-13 11:51:15 [INFO]             },\n",
      "2025-05-13 11:51:15 [INFO]             'layer_wise_quant': False,\n",
      "2025-05-13 11:51:15 [INFO]             'layer_wise_quant_args': {\n",
      "2025-05-13 11:51:15 [INFO]             },\n",
      "2025-05-13 11:51:15 [INFO]             'fast_bias_correction': False,\n",
      "2025-05-13 11:51:15 [INFO]             'weight_correction': False,\n",
      "2025-05-13 11:51:15 [INFO]             'gemm_to_matmul': True,\n",
      "2025-05-13 11:51:15 [INFO]             'graph_optimization_level': None,\n",
      "2025-05-13 11:51:15 [INFO]             'first_conv_or_matmul_quantization': True,\n",
      "2025-05-13 11:51:15 [INFO]             'last_conv_or_matmul_quantization': True,\n",
      "2025-05-13 11:51:15 [INFO]             'pre_post_process_quantization': True,\n",
      "2025-05-13 11:51:15 [INFO]             'add_qdq_pair_to_weight': False,\n",
      "2025-05-13 11:51:15 [INFO]             'optypes_to_exclude_output_quant': [\n",
      "2025-05-13 11:51:15 [INFO]             ],\n",
      "2025-05-13 11:51:15 [INFO]             'dedicated_qdq_pair': False,\n",
      "2025-05-13 11:51:15 [INFO]             'rtn_args': {\n",
      "2025-05-13 11:51:15 [INFO]             },\n",
      "2025-05-13 11:51:15 [INFO]             'awq_args': {\n",
      "2025-05-13 11:51:15 [INFO]             },\n",
      "2025-05-13 11:51:15 [INFO]             'gptq_args': {\n",
      "2025-05-13 11:51:15 [INFO]             },\n",
      "2025-05-13 11:51:15 [INFO]             'teq_args': {\n",
      "2025-05-13 11:51:15 [INFO]             },\n",
      "2025-05-13 11:51:15 [INFO]             'autoround_args': {\n",
      "2025-05-13 11:51:15 [INFO]             }\n",
      "2025-05-13 11:51:15 [INFO]         },\n",
      "2025-05-13 11:51:15 [INFO]         'reduce_range': None,\n",
      "2025-05-13 11:51:15 [INFO]         'TuningCriterion': {\n",
      "2025-05-13 11:51:15 [INFO]             'max_trials': 100,\n",
      "2025-05-13 11:51:15 [INFO]             'objective': [\n",
      "2025-05-13 11:51:15 [INFO]                 'performance'\n",
      "2025-05-13 11:51:15 [INFO]             ],\n",
      "2025-05-13 11:51:15 [INFO]             'strategy': 'basic',\n",
      "2025-05-13 11:51:15 [INFO]             'strategy_kwargs': None,\n",
      "2025-05-13 11:51:15 [INFO]             'timeout': 0\n",
      "2025-05-13 11:51:15 [INFO]         },\n",
      "2025-05-13 11:51:15 [INFO]         'use_bf16': True,\n",
      "2025-05-13 11:51:15 [INFO]         'ni_workload_name': 'quantization'\n",
      "2025-05-13 11:51:15 [INFO]     }\n",
      "2025-05-13 11:51:15 [INFO] }\n",
      "2025-05-13 11:51:15 [WARNING] [Strategy] Please install `mpi4py` correctly if using distributed tuning; otherwise, ignore this warning.\n",
      "2025-05-13 11:51:15 [WARNING] The model is automatically detected as a non-NLP model. You can use 'domain' argument in 'PostTrainingQuantConfig' to overwrite it\n",
      "2025-05-13 11:51:15 [WARNING] Graph optimization level is automatically set to ENABLE_BASIC. You can use 'recipe' argument in 'PostTrainingQuantConfig'to overwrite it\n",
      "2025-05-13 11:51:16 [INFO] Do not evaluate the baseline and quantize the model with default configuration.\n",
      "2025-05-13 11:51:16 [INFO] Quantize the model with default config.\n",
      "2025-05-13 11:51:18 [INFO] |**********Mixed Precision Statistics*********|\n",
      "2025-05-13 11:51:18 [INFO] +-----------------------+-------+------+------+\n",
      "2025-05-13 11:51:18 [INFO] |        Op Type        | Total | INT8 | FP32 |\n",
      "2025-05-13 11:51:18 [INFO] +-----------------------+-------+------+------+\n",
      "2025-05-13 11:51:18 [INFO] |         MatMul        |   10  |  6   |  4   |\n",
      "2025-05-13 11:51:18 [INFO] |          Conv         |   4   |  4   |  0   |\n",
      "2025-05-13 11:51:18 [INFO] |         Gather        |   19  |  3   |  16  |\n",
      "2025-05-13 11:51:18 [INFO] |    DequantizeLinear   |   3   |  3   |  0   |\n",
      "2025-05-13 11:51:18 [INFO] | DynamicQuantizeLinear |   10  |  10  |  0   |\n",
      "2025-05-13 11:51:18 [INFO] +-----------------------+-------+------+------+\n",
      "2025-05-13 11:51:18 [INFO] Pass quantize model elapsed time: 1825.97 ms\n",
      "2025-05-13 11:51:18 [INFO] Save tuning history to /home/jovyan/work/nc_workspace/2025-05-13_11-43-51/./history.snapshot.\n",
      "2025-05-13 11:51:18 [INFO] [Strategy] Found the model meets accuracy requirements, ending the tuning process.\n",
      "2025-05-13 11:51:18 [INFO] Specified timeout or max trials is reached! Found a quantized model which meet accuracy goal. Exit.\n",
      "2025-05-13 11:51:18 [INFO] Save deploy yaml to /home/jovyan/work/nc_workspace/2025-05-13_11-43-51/deploy.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model saved to: models/SSE_PT10kemb_quant_dynamic.onnx\n",
      "Quantized Model Size on Disk: 18.85 MB\n",
      "Execution provider: ['CPUExecutionProvider']\n",
      "Inference Latency (median): 4.93 ms\n",
      "Inference Latency (95th): 5.08 ms\n",
      "Inference Latency (99th): 5.36 ms\n",
      "Inference Throughput (single sample): 202.33 FPS\n",
      "Batch Throughput: 940.05 FPS\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import neural_compressor\n",
    "from neural_compressor import quantization\n",
    "from neural_compressor.config import PostTrainingQuantConfig\n",
    "from neural_compressor.model.onnx_model import ONNXModel\n",
    "\n",
    "fp32_model_path = \"models/SSE_PT10kemb.onnx\"\n",
    "fp32_model = ONNXModel(fp32_model_path)\n",
    "\n",
    "config = PostTrainingQuantConfig(approach=\"dynamic\")\n",
    "\n",
    "q_model = quantization.fit(\n",
    "    model=fp32_model,\n",
    "    conf=config\n",
    ")\n",
    "\n",
    "quantized_model_path = \"models/SSE_PT10kemb_quant_dynamic.onnx\"\n",
    "q_model.save_model_to_file(quantized_model_path)\n",
    "print(f\"Quantized model saved to: {quantized_model_path}\")\n",
    "\n",
    "model_size = os.path.getsize(quantized_model_path)\n",
    "print(f\"Quantized Model Size on Disk: {model_size / 1e6:.2f} MB\")\n",
    "\n",
    "ort_session = ort.InferenceSession(quantized_model_path, providers=['CPUExecutionProvider'])\n",
    "benchmark_session(ort_session)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- \n",
    "\n",
    "On liqid AMD EPYC\n",
    "\n",
    "Model Size on Disk: 2.42 MB\n",
    "Execution provider: ['CPUExecutionProvider']\n",
    "Accuracy: 82.04% (2746/3347 correct)\n",
    "Inference Latency (single sample, median): 22.32 ms\n",
    "Inference Latency (single sample, 95th percentile): 22.97 ms\n",
    "Inference Latency (single sample, 99th percentile): 23.14 ms\n",
    "Inference Throughput (single sample): 44.71 FPS\n",
    "Batch Throughput: 38.34 FPS\n",
    "\n",
    "On liqid Intel\n",
    "\n",
    "Execution provider: ['CPUExecutionProvider']\n",
    "Accuracy: 84.58% (2831/3347 correct)\n",
    "Inference Latency (single sample, median): 28.29 ms\n",
    "Inference Latency (single sample, 95th percentile): 29.00 ms\n",
    "Inference Latency (single sample, 99th percentile): 29.07 ms\n",
    "Inference Throughput (single sample): 35.28 FPS\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Static quantization\n",
    "\n",
    "Next, we will try static quantization with a calibration dataset.\n",
    "\n",
    "First, let’s prepare the calibration dataset. This dataset will also be used to evaluate the quantized model, to see if it meets the accuracy criterion we will set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_ids shape: (64, 1), item_seqs shape: (64, 100)\n",
      "Returned keys: dict_keys(['user_ids', 'item_seqs'])\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    user_batch = np.vstack([b[\"user_ids\"] for b in batch]).astype(np.int64)\n",
    "    seq_batch = np.vstack([b[\"item_seqs\"] for b in batch]).astype(np.int64)\n",
    "    print(f\"user_ids shape: {user_batch.shape}, item_seqs shape: {seq_batch.shape}\")\n",
    "    return {\n",
    "        \"user_ids\": user_batch,\n",
    "        \"item_seqs\": seq_batch\n",
    "    }\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "calib_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "sample = next(iter(calib_loader))\n",
    "print(\"Returned keys:\", sample.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_compressor.config import PostTrainingQuantConfig\n",
    "\n",
    "dataset = SequentialEvalDataset(\"/mnt/data/evaluation/movielens_192m_eval.txt\", seq_max_len=100)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    user_batch = np.vstack([b[\"user_ids\"] for b in batch]).astype(np.int64)\n",
    "    seq_batch = np.vstack([b[\"item_seqs\"] for b in batch]).astype(np.int64)\n",
    "    print(f\"user_ids shape: {user_batch.shape}, item_seqs shape: {seq_batch.shape}\")\n",
    "    return {\n",
    "        \"user_ids\": user_batch,\n",
    "        \"item_seqs\": seq_batch\n",
    "    }\n",
    "\n",
    "eval_loader = DataLoader(dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "fp32_model_path = \"models/SSE_PT10kemb.onnx\"\n",
    "fp32_model = ONNXModel(fp32_model_path)\n",
    "\n",
    "config_ptq = PostTrainingQuantConfig(\n",
    "    approach=\"static\",\n",
    "    device=\"cpu\",\n",
    "    quant_level=1,\n",
    "    quant_format=\"QOperator\",\n",
    "    recipes={\"graph_optimization_level\": \"ENABLE_EXTENDED\"},\n",
    "    calibration_sampling_size=128\n",
    ")\n",
    "\n",
    "q_model = quantization.fit(\n",
    "    model=fp32_model,\n",
    "    conf=config_ptq,\n",
    "    calib_dataloader=eval_loader  \n",
    ")\n",
    "\n",
    "quantized_model_path = \"models/SSE_PT10kemb_quant_static.onnx\"\n",
    "if q_model:\n",
    "    q_model.save_model_to_file(quantized_model_path)\n",
    "    print(f\"Static quantized model saved to: {quantized_model_path}\")\n",
    "\n",
    "    model_size = os.path.getsize(quantized_model_path)\n",
    "    print(f\"Quantized Model Size on Disk: {model_size / 1e6:.2f} MB\")\n",
    "\n",
    "    ort_session = ort.InferenceSession(quantized_model_path, providers=[\"CPUExecutionProvider\"])\n",
    "    benchmark_session(ort_session)\n",
    "else:\n",
    "    print(\"Quantization fail\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre style=\"font-size:84%; line-height:1.3em; font-family:monospace;\">\n",
    "Model Size on Disk: 6.01 MB\n",
    "Accuracy: 90.20% (3019/3347 correct)\n",
    "Inference Latency (single sample, median): 10.20 ms\n",
    "Inference Latency (single sample, 95th percentile): 10.39 ms\n",
    "Inference Latency (single sample, 99th percentile): 10.66 ms\n",
    "Inference Throughput (single sample): 97.87 FPS\n",
    "Batch Throughput: 277.23 FPS\n",
    "\n",
    "On intel\n",
    "\n",
    "Execution provider: ['CPUExecutionProvider']\n",
    "Accuracy: 90.44% (3027/3347 correct)\n",
    "Inference Latency (single sample, median): 6.60 ms\n",
    "Inference Latency (single sample, 95th percentile): 6.66 ms\n",
    "Inference Latency (single sample, 99th percentile): 6.68 ms\n",
    "Inference Throughput (single sample): 151.36 FPS\n",
    "Batch Throughput: 540.19 FPS\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "\n",
    "on AMD EPYC\n",
    "\n",
    "Model Size on Disk: 6.01 MB\n",
    "Accuracy: 90.20% (3019/3347 correct)\n",
    "Inference Latency (single sample, median): 10.20 ms\n",
    "Inference Latency (single sample, 95th percentile): 10.39 ms\n",
    "Inference Latency (single sample, 99th percentile): 10.66 ms\n",
    "Inference Throughput (single sample): 97.87 FPS\n",
    "Batch Throughput: 277.23 FPS\n",
    "\n",
    "On intel\n",
    "\n",
    "Execution provider: ['CPUExecutionProvider']\n",
    "Accuracy: 90.44% (3027/3347 correct)\n",
    "Inference Latency (single sample, median): 6.60 ms\n",
    "Inference Latency (single sample, 95th percentile): 6.66 ms\n",
    "Inference Latency (single sample, 99th percentile): 6.68 ms\n",
    "Inference Throughput (single sample): 151.36 FPS\n",
    "Batch Throughput: 540.19 FPS\n",
    "\n",
    "-->\n",
    "<!--\n",
    "\n",
    "\n",
    "::: {.cell .markdown}\n",
    "\n",
    "### Quantization aware training\n",
    "\n",
    "To achieve the best of both worlds - high accuracy, but the small model size and faster inference time of a quantized model - we can try quantization aware training. In QAT, the effect of quantization is \"simulated\" during training, so that we learn weights that are more robust to quantization. Then, when we quantize the model, we can achieve better accuracy.\n",
    "\n",
    ":::\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
