{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure inference performance of PyTorch model on CPU\n",
    "\n",
    "First, we are going to measure the inference performance of an already-trained PyTorch model on CPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchinfo import summary\n",
    "import time\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import get_max_item_id, build_model_from_ckpt\n",
    "from utilities import SSEPT\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "max_item_id = get_max_item_id(\"/mnt/data/evaluation/movielens_192m_eval.txt\")\n",
    "\n",
    "model = build_model_from_ckpt(\"SSE_PT10kemb.pth\", DEVICE, item_num=max_item_id)\n",
    "model = torch.compile(model)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre style=\"font-size:84%; line-height:1.3em; font-family:monospace;\">\n",
    "OptimizedModule(\n",
    "  (_orig_mod): SSEPT(\n",
    "    (user_embedding): Embedding(10001, 100)\n",
    "    (item_embedding): Embedding(84433, 100, padding_idx=0)\n",
    "    (position_embedding): Embedding(100, 100)\n",
    "    (dropout): Dropout(p=0.5, inplace=False)\n",
    "    (blocks): ModuleList(\n",
    "      (0-1): 2 x TransformerBlock(\n",
    "        (attention): MultiheadAttention(\n",
    "          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
    "        )\n",
    "        (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
    "        (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
    "        (ffn): PointWiseFeedForward(\n",
    "          (conv1): Conv1d(100, 100, kernel_size=(1,), stride=(1,))\n",
    "          (conv2): Conv1d(100, 100, kernel_size=(1,), stride=(1,))\n",
    "          (dropout): Dropout(p=0.5, inplace=False)\n",
    "          (activation): ReLU()\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "    (final_linear): Linear(in_features=200, out_features=100, bias=True)\n",
    "    (output_layer): Linear(in_features=100, out_features=84432, bias=True)\n",
    "  )\n",
    ")\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and also prepare our test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1374159 valid sequences from /mnt/data/evaluation/movielens_192m_eval.txt\n"
     ]
    }
   ],
   "source": [
    "from utilities import pad_or_truncate\n",
    "\n",
    "class SequentialEvalDataset(Dataset):\n",
    "    def __init__(self, filepath, seq_max_len):\n",
    "        self.user_sequences = {}\n",
    "        with open(filepath, \"r\") as f:\n",
    "            for line in f:\n",
    "                uid, iid = map(int, line.strip().split(\"\\t\"))\n",
    "                self.user_sequences.setdefault(uid, []).append(iid)\n",
    "\n",
    "        # 构造 (user_id, sequence, label)\n",
    "        self.samples = []\n",
    "        self.seq_max_len = seq_max_len\n",
    "        for uid, seq in self.user_sequences.items():\n",
    "            if len(seq) < 2:\n",
    "                continue\n",
    "            self.samples.append((uid, seq[:-1], seq[-1]))\n",
    "\n",
    "        print(f\"Loaded {len(self.samples)} valid sequences from {filepath}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        uid, seq, label = self.samples[idx]\n",
    "        seq_tensor = torch.tensor(pad_or_truncate(seq, self.seq_max_len), dtype=torch.long)\n",
    "        return torch.tensor(uid, dtype=torch.long), seq_tensor, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# 加载数据\n",
    "seq_max_len = model.seq_max_len\n",
    "dataset = SequentialEvalDataset(\"/mnt/data/evaluation/movielens_192m_eval.txt\", seq_max_len)\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max item id in batch: 77051\n",
      "Embedding size: 84433\n"
     ]
    }
   ],
   "source": [
    "user_batch, seq_batch, _ = next(iter(loader))\n",
    "\n",
    "print(\"Max item id in batch:\", seq_batch.max().item())\n",
    "print(\"Embedding size:\", model.item_embedding.num_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will measure:\n",
    "\n",
    "-   the size of the model on disk\n",
    "-   the latency when doing inference on single samples\n",
    "-   the throughput when doing inference on batches of data\n",
    "-   and the test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model size\n",
    "\n",
    "We’ll start with model size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size on Disk: 5.56 MB\n"
     ]
    }
   ],
   "source": [
    "model_size = os.path.getsize(MODEL_PATH) \n",
    "print(f\"Model Size on Disk: {model_size/ (1e6) :.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference latency\n",
    "\n",
    "Now, we’ll measure how long it takes the model to return a prediction for a single sample. We will run 100 trials, and then compute aggregate statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0513 10:08:59.206000 44643 site-packages/torch/_dynamo/variables/tensor.py:776] [0/0] Graph break from `Tensor.item()`, consider setting:\n",
      "W0513 10:08:59.206000 44643 site-packages/torch/_dynamo/variables/tensor.py:776] [0/0]     torch._dynamo.config.capture_scalar_outputs = True\n",
      "W0513 10:08:59.206000 44643 site-packages/torch/_dynamo/variables/tensor.py:776] [0/0] or:\n",
      "W0513 10:08:59.206000 44643 site-packages/torch/_dynamo/variables/tensor.py:776] [0/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1\n",
      "W0513 10:08:59.206000 44643 site-packages/torch/_dynamo/variables/tensor.py:776] [0/0] to include these operations in the captured graph.\n",
      "W0513 10:08:59.206000 44643 site-packages/torch/_dynamo/variables/tensor.py:776] [0/0] \n",
      "W0513 10:08:59.206000 44643 site-packages/torch/_dynamo/variables/tensor.py:776] [0/0] Graph break: from user code at:\n",
      "W0513 10:08:59.206000 44643 site-packages/torch/_dynamo/variables/tensor.py:776] [0/0]   File \"/home/jovyan/work/utilities.py\", line 92, in forward\n",
      "W0513 10:08:59.206000 44643 site-packages/torch/_dynamo/variables/tensor.py:776] [0/0]     assert item_seqs.max().item() < self.item_embedding.num_embeddings, \\\n",
      "W0513 10:08:59.206000 44643 site-packages/torch/_dynamo/variables/tensor.py:776] [0/0] \n",
      "W0513 10:08:59.206000 44643 site-packages/torch/_dynamo/variables/tensor.py:776] [0/0] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single sample latency median: 3.19 ms\n",
      "Inference Latency (median): 3.19 ms\n",
      "Inference Latency (95th): 3.63 ms\n",
      "Inference Latency (99th): 112.37 ms\n",
      "Throughput (single sample): 9.86 FPS\n"
     ]
    }
   ],
   "source": [
    "uid, seq_tensor, _ = dataset[0]  \n",
    "user_tensor = uid.unsqueeze(0).to(DEVICE)\n",
    "seq_tensor = seq_tensor.unsqueeze(0).to(DEVICE)\n",
    "\n",
    "latencies = []\n",
    "for _ in range(100):\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        _ = model(user_tensor, seq_tensor)\n",
    "    latencies.append(time.time() - start)\n",
    "lat_ms = [t * 1000 for t in latencies]\n",
    "print(f\"Single sample latency median: {torch.median(torch.tensor(lat_ms)):.2f} ms\")\n",
    "\n",
    "print(f\"Inference Latency (median): {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (95th): {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (99th): {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "print(f\"Throughput (single sample): {100 / np.sum(latencies):.2f} FPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch throughput\n",
    "\n",
    "Finally, we’ll measure the rate at which the model can return predictions for batches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Throughput: 5381.72 FPS\n"
     ]
    }
   ],
   "source": [
    "num_batches = 50 \n",
    "\n",
    "# 从 evaluation loader 中取一个 batch\n",
    "user_batch, seq_batch, _ = next(iter(loader))\n",
    "user_batch = user_batch.to(DEVICE)\n",
    "seq_batch = seq_batch.to(DEVICE)\n",
    "\n",
    "# Warm-up run \n",
    "with torch.no_grad():\n",
    "    _ = model(user_batch, seq_batch)\n",
    "\n",
    "batch_times = []\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_batches):\n",
    "        start_time = time.time()\n",
    "        _ = model(user_batch, seq_batch)\n",
    "        batch_times.append(time.time() - start_time)\n",
    "\n",
    "batch_fps = (user_batch.shape[0] * num_batches) / np.sum(batch_times) \n",
    "print(f\"Batch Throughput: {batch_fps:.2f} FPS\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are done, download the fully executed notebook from the Jupyter container environment for later reference. (Note: because it is an executable file, and you are downloading it from a site that is not secured with HTTPS, you may have to explicitly confirm the download in some browsers.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eager mode execution vs compiled model\n",
    "\n",
    "We had just evaluated a model in eager mode. However, in some (although, not all) cases we may get better performance from compiling the model into a graph, and executing it as a graph.\n",
    "\n",
    "Go back to the cell where the model is loaded, and add\n",
    "\n",
    "``` python\n",
    "model.compile()\n",
    "```\n",
    "\n",
    "just below the call to `torch.load`. Then, run the notebook again (“Run \\> Run All Cells”).\n",
    "\n",
    "When you are done, download the fully executed notebook **again** from the Jupyter container environment for later reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- \n",
    "\n",
    "compute_gigaio \n",
    "\n",
    "  Model name:             AMD EPYC 7763 64-Core Processor\n",
    "    CPU family:           25\n",
    "    Model:                1\n",
    "    Thread(s) per core:   2\n",
    "    Core(s) per socket:   64\n",
    "\n",
    "-->\n",
    "<!-- summary for mobilenet model\n",
    "\n",
    "Model Size on Disk: 9.23 MB\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 60.16 ms\n",
    "Inference Latency (single sample, 95th percentile): 77.22 ms\n",
    "Inference Latency (single sample, 99th percentile): 77.37 ms\n",
    "Inference Throughput (single sample): 15.82 FPS\n",
    "Batch Throughput: 83.66 FPS\n",
    "\n",
    "\n",
    "Model Size on Disk: 9.23 MB\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 73.97 ms\n",
    "Inference Latency (single sample, 95th percentile): 83.16 ms\n",
    "Inference Latency (single sample, 99th percentile): 83.94 ms\n",
    "Inference Throughput (single sample): 13.34 FPS\n",
    "Batch Throughput: 98.80 FPS\n",
    "\n",
    "-->\n",
    "<!-- summary for mobilenet compiled model\n",
    "\n",
    "Model Size on Disk: 9.23 MB\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 26.92 ms\n",
    "Inference Latency (single sample, 95th percentile): 49.79 ms\n",
    "Inference Latency (single sample, 99th percentile): 64.55 ms\n",
    "Inference Throughput (single sample): 32.35 FPS\n",
    "Batch Throughput: 249.08 FPS\n",
    "\n",
    "Model Size on Disk: 9.23 MB\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 34.14 ms\n",
    "Inference Latency (single sample, 95th percentile): 53.85 ms\n",
    "Inference Latency (single sample, 99th percentile): 60.23 ms\n",
    "Inference Throughput (single sample): 27.39 FPS\n",
    "Batch Throughput: 281.65 FPS\n",
    "\n",
    "-->\n",
    "<!-- \n",
    "\n",
    "(Intel CPU)\n",
    "\n",
    "Model Size on Disk: 9.23 MB\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 12.69 ms\n",
    "Inference Latency (single sample, 95th percentile): 12.83 ms\n",
    "Inference Latency (single sample, 99th percentile): 12.97 ms\n",
    "Inference Throughput (single sample): 78.73 FPS\n",
    "Batch Throughput: 161.27 FPS\n",
    "\n",
    "With compiling\n",
    "\n",
    "Model Size on Disk: 9.23 MB\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 8.47 ms\n",
    "Inference Latency (single sample, 95th percentile): 8.58 ms\n",
    "Inference Latency (single sample, 99th percentile): 8.79 ms\n",
    "Inference Throughput (single sample): 117.86 FPS\n",
    "Batch Throughput: 474.67 FPS\n",
    "\n",
    "\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
