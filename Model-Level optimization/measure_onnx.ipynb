{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure inference performance of ONNX model on CPU\n",
    "\n",
    "To squeeze even more inference performance out of our model, we are going to convert it to ONNX format, which allows models from different frameworks (PyTorch, Tensorflow, Keras), to be deployed on a variety of different hardware platforms (CPU, GPU, edge devices), using many optimizations (graph optimizations, quantization, target device-specific implementations, and more)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from utilities import build_model_from_ckpt, pad_or_truncate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = \"models/SSE_PT10kemb.onnx\"\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "ort_session = ort.InferenceSession(onnx_model_path, providers=['CPUExecutionProvider'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let’s load our saved PyTorch model, and convert it to ONNX using PyTorch’s built-in `torch.onnx.export`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Entered __init__\n",
      "Loaded 1374159 valid sequences from /mnt/data/evaluation/movielens_192m_eval.txt\n"
     ]
    }
   ],
   "source": [
    "from utilities import get_max_item_id\n",
    "\n",
    "class SequentialEvalDataset(Dataset):\n",
    "    def __init__(self, filepath, seq_max_len, return_label=True):\n",
    "        print(\">>> Entered __init__\")\n",
    "\n",
    "        self.user_sequences = {}\n",
    "        with open(filepath, \"r\") as f:\n",
    "            for line in f:\n",
    "                uid, iid = map(int, line.strip().split(\"\\t\"))\n",
    "                self.user_sequences.setdefault(uid, []).append(iid)          \n",
    "\n",
    "        self.samples = []\n",
    "        self.seq_max_len = seq_max_len\n",
    "        self.return_label = return_label\n",
    "\n",
    "        for uid, seq in self.user_sequences.items():\n",
    "            if len(seq) < 2:\n",
    "                continue\n",
    "            sequence = seq[:-1]\n",
    "            label = seq[-1]\n",
    "            self.samples.append((uid, sequence, label))\n",
    "\n",
    "        print(f\"Loaded {len(self.samples)} valid sequences from {filepath}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        uid, seq, label = self.samples[idx]\n",
    "        seq_tensor = torch.tensor(pad_or_truncate(seq, self.seq_max_len), dtype=torch.long)\n",
    "        uid_tensor = torch.tensor(uid, dtype=torch.long)\n",
    "        if self.return_label:\n",
    "            return uid_tensor, seq_tensor, torch.tensor(label, dtype=torch.long)\n",
    "        else:\n",
    "            return uid_tensor, seq_tensor\n",
    "\n",
    "seq_max_len = 100\n",
    "dataset = SequentialEvalDataset(\"/mnt/data/evaluation/movielens_192m_eval.txt\", seq_max_len, return_label=False)\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an inference session\n",
    "\n",
    "Now, we can evaluate our model! To use an ONNX model, we create an *inference session*, and then use the model within that session. Let’s start an inference session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: [<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7e78cc2bcb70>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7e78cc2bf730>]\n",
      "Outputs: [<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x7e78cc2bc630>]\n"
     ]
    }
   ],
   "source": [
    "onnx_model = onnx.load(onnx_model_path)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "ort_session = ort.InferenceSession(onnx_model_path, providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "print(\"Inputs:\", ort_session.get_inputs())\n",
    "print(\"Outputs:\", ort_session.get_outputs())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model size\n",
    "\n",
    "We are also concerned with the size of the ONNX model on disk. It will be similar to the equivalent PyTorch model size (to start!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size on Disk: 72.53 MB\n"
     ]
    }
   ],
   "source": [
    "model_size = os.path.getsize(onnx_model_path) \n",
    "print(f\"Model Size on Disk: {model_size/ (1e6) :.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference latency\n",
    "\n",
    "Now, we’ll measure how long it takes the model to return a prediction for a single sample. We will run 100 trials, and then compute aggregate statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Latency (single sample, median): 2.05 ms\n",
      "Inference Latency (single sample, 95th percentile): 3.86 ms\n",
      "Inference Latency (single sample, 99th percentile): 4.46 ms\n",
      "Inference Throughput (single sample): 430.97 FPS\n"
     ]
    }
   ],
   "source": [
    "num_trials = 100  \n",
    "\n",
    "user_tensor, seq_tensor = next(iter(loader))\n",
    "user_input = user_tensor[:1].numpy()\n",
    "seq_input = seq_tensor[:1].numpy()\n",
    "\n",
    "# Warm-up\n",
    "ort_session.run(None, {\n",
    "    ort_session.get_inputs()[0].name: user_input,\n",
    "    ort_session.get_inputs()[1].name: seq_input\n",
    "})\n",
    "\n",
    "latencies = []\n",
    "for _ in range(num_trials):\n",
    "    start = time.time()\n",
    "    ort_session.run(None, {\n",
    "        ort_session.get_inputs()[0].name: user_input,\n",
    "        ort_session.get_inputs()[1].name: seq_input\n",
    "    })\n",
    "    latencies.append(time.time() - start)\n",
    "\n",
    "print(f\"Inference Latency (single sample, median): {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (single sample, 95th percentile): {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (single sample, 99th percentile): {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "print(f\"Inference Throughput (single sample): {num_trials / np.sum(latencies):.2f} FPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch throughput\n",
    "\n",
    "Finally, we’ll measure the rate at which the model can return predictions for batches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Throughput: 914.38 FPS\n"
     ]
    }
   ],
   "source": [
    "num_batches = 50  \n",
    "\n",
    "user_tensor, seq_tensor = next(iter(loader))\n",
    "user_input = user_tensor.numpy()\n",
    "seq_input = seq_tensor.numpy()\n",
    "\n",
    "# Warm-up\n",
    "ort_session.run(None, {\n",
    "    ort_session.get_inputs()[0].name: user_input,\n",
    "    ort_session.get_inputs()[1].name: seq_input\n",
    "})\n",
    "\n",
    "batch_times = []\n",
    "for _ in range(num_batches):\n",
    "    start_time = time.time()\n",
    "    ort_session.run(None, {\n",
    "        ort_session.get_inputs()[0].name: user_input,\n",
    "        ort_session.get_inputs()[1].name: seq_input\n",
    "    })\n",
    "    batch_times.append(time.time() - start_time)\n",
    "\n",
    "batch_fps = (user_input.shape[0] * num_batches) / np.sum(batch_times)\n",
    "print(f\"Batch Throughput: {batch_fps:.2f} FPS\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size on Disk: 72.53 MB\n",
      "Inference Latency (single sample, median): 2.05 ms\n",
      "Inference Latency (single sample, 95th percentile): 3.86 ms\n",
      "Inference Latency (single sample, 99th percentile): 4.46 ms\n",
      "Inference Throughput (single sample): 430.97 FPS\n",
      "Batch Throughput: 914.38 FPS\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model Size on Disk: {model_size/ (1e6) :.2f} MB\")\n",
    "print(f\"Inference Latency (single sample, median): {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (single sample, 95th percentile): {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (single sample, 99th percentile): {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "print(f\"Inference Throughput (single sample): {num_trials / np.sum(latencies):.2f} FPS\")\n",
    "print(f\"Batch Throughput: {batch_fps:.2f} FPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- summary for mobilenet\n",
    "\n",
    "Model Size on Disk: 8.92 MB\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 8.92 ms\n",
    "Inference Latency (single sample, 95th percentile): 9.15 ms\n",
    "Inference Latency (single sample, 99th percentile): 9.41 ms\n",
    "Inference Throughput (single sample): 112.06 FPS\n",
    "Batch Throughput: 993.48 FPS\n",
    "\n",
    "Model Size on Disk: 8.92 MB\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 9.64 ms\n",
    "Inference Latency (single sample, 95th percentile): 10.57 ms\n",
    "Inference Latency (single sample, 99th percentile): 11.72 ms\n",
    "Inference Latency (single sample, std error): 0.04 ms\n",
    "Inference Throughput (single sample): 102.52 FPS\n",
    "Batch Throughput: 1083.57 FPS\n",
    "\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Model Size on Disk: 8.92 MB\n",
    "Inference Latency (single sample, median): 16.24 ms\n",
    "Inference Latency (single sample, 95th percentile): 18.06 ms\n",
    "Inference Latency (single sample, 99th percentile): 18.72 ms\n",
    "Inference Throughput (single sample): 63.51 FPS\n",
    "Batch Throughput: 1103.28 FPS\n",
    "\n",
    "\n",
    "-->\n",
    "<!-- summary for mobilenet with graph optimization\n",
    "\n",
    "Model Size on Disk: 8.91 MB\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 9.31 ms\n",
    "Inference Latency (single sample, 95th percentile): 9.47 ms\n",
    "Inference Latency (single sample, 99th percentile): 9.71 ms\n",
    "Inference Throughput (single sample): 107.22 FPS\n",
    "Batch Throughput: 1091.58 FPS\n",
    "\n",
    "Model Size on Disk: 8.91 MB\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 9.95 ms\n",
    "Inference Latency (single sample, 95th percentile): 10.14 ms\n",
    "Inference Latency (single sample, 99th percentile): 10.70 ms\n",
    "Inference Latency (single sample, std error): 0.02 ms\n",
    "Inference Throughput (single sample): 100.18 FPS\n",
    "Batch Throughput: 1022.77 FPS\n",
    "\n",
    "Model Size on Disk: 8.91 MB\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 9.55 ms\n",
    "Inference Latency (single sample, 95th percentile): 10.58 ms\n",
    "Inference Latency (single sample, 99th percentile): 11.14 ms\n",
    "Inference Latency (single sample, std error): 0.04 ms\n",
    "Inference Throughput (single sample): 102.97 FPS\n",
    "Batch Throughput: 1079.81 FPS\n",
    "\n",
    "\n",
    "-->\n",
    "<!-- \n",
    "\n",
    "(Intel CPU)\n",
    "\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Model Size on Disk: 8.92 MB\n",
    "Inference Latency (single sample, median): 4.53 ms\n",
    "Inference Latency (single sample, 95th percentile): 4.63 ms\n",
    "Inference Latency (single sample, 99th percentile): 4.99 ms\n",
    "Inference Throughput (single sample): 218.75 FPS\n",
    "Batch Throughput: 2519.80 FPS\n",
    "\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are done, download the fully executed notebook from the Jupyter container environment for later reference. (Note: because it is an executable file, and you are downloading it from a site that is not secured with HTTPS, you may have to explicitly confirm the download in some browsers.)\n",
    "\n",
    "Also download the `food11.onnx` model from inside the `models` directory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
