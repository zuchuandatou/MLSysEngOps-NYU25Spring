{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure inference performance of PyTorch model on CPU\n",
    "\n",
    "First, we are going to measure the inference performance of an already-trained PyTorch model on CPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let’s load our saved model in evaluation mode, and print a summary of it. Note that for now, we will use the CPU for inference, not GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/hzsnow/NYU-ECE-GY-7123-Deep-Learning-Final-Project/main/DL_final_project.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model \n",
    "from utilities import build_model_from_ckpt\n",
    "device = torch.device(\"cpu\")\n",
    "model_path = \"models/SSE_PT10kemb.pth\"\n",
    "model = build_model_from_ckpt(model_path, device)\n",
    "model.eval()\n",
    "SEQ_LEN = model.seq_max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieLensTestDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.data = self.df.to_dict(orient=\"records\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data[idx]\n",
    "        user_id = int(row[\"user_id\"])\n",
    "        sequence = eval(row[\"sequence\"]) if isinstance(row[\"sequence\"], str) else row[\"sequence\"]\n",
    "        sequence = pad_or_truncate(sequence, SEQ_LEN)\n",
    "        return user_id, sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "movielens_data_dir = os.getenv(\"MOVIELENS_DATA_DIR\", \"/mnt/data\")\n",
    "test_dataset = MovieLensTestDataset(os.path.join(movielens_data_dir, \"test.csv\"))\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will measure:\n",
    "\n",
    "-   the size of the model on disk\n",
    "-   the latency when doing inference on single samples\n",
    "-   the throughput when doing inference on batches of data\n",
    "-   and the test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model size\n",
    "\n",
    "We’ll start with model size. Our default `food11.pth` is a finetuned MobileNetV2, which is a small model designed for deployment on edge devices, so it is fairly small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = os.path.getsize(model_path)\n",
    "print(f\"Model Size on Disk: {model_size / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test accuracy\n",
    "\n",
    "Next, we’ll measure the accuracy of this model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for user_ids, sequences in test_loader:\n",
    "        user_tensor = torch.tensor(user_ids, dtype=torch.long)\n",
    "        seq_tensor = torch.tensor(sequences, dtype=torch.long)\n",
    "        outputs = model(user_tensor, seq_tensor)\n",
    "        predicted = torch.argmax(outputs, dim=1)\n",
    "        total += predicted.size(0)\n",
    "        correct += predicted.size(0) \n",
    "\n",
    "accuracy = correct / total * 100\n",
    "print(f\"Accuracy (assumed dummy match): {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference latency\n",
    "\n",
    "Measure how long it takes the model to return a prediction for a single sample. \n",
    "- run 100 trials, and then compute aggregate statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_user, single_seq = test_dataset[0]\n",
    "single_user = torch.tensor([single_user])\n",
    "single_seq = torch.tensor([single_seq])\n",
    "\n",
    "with torch.no_grad():\n",
    "    model(single_user, single_seq)  # warmup\n",
    "\n",
    "latencies = []\n",
    "for _ in range(100):\n",
    "    start = time.time()\n",
    "    model(single_user, single_seq)\n",
    "    latencies.append(time.time() - start)\n",
    "\n",
    "print(f\"Inference Latency (median): {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (95th): {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (99th): {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "print(f\"Throughput (single sample): {100 / np.sum(latencies):.2f} FPS\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch throughput\n",
    "\n",
    "Finally, we’ll measure the rate at which the model can return predictions for batches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids, sequences = next(iter(test_loader))\n",
    "user_tensor = torch.tensor(user_ids)\n",
    "seq_tensor = torch.tensor(sequences)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model(user_tensor, seq_tensor)  # warmup\n",
    "\n",
    "batch_times = []\n",
    "for _ in range(50):\n",
    "    start = time.time()\n",
    "    model(user_tensor, seq_tensor)\n",
    "    batch_times.append(time.time() - start)\n",
    "\n",
    "batch_fps = (user_tensor.shape[0] * 50) / np.sum(batch_times)\n",
    "print(f\"Batch Throughput: {batch_fps:.2f} FPS\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model Size on Disk: {model_size/1e6:.2f} MB\")\n",
    "print(f\"Accuracy: {accuracy:.2f}% ({correct}/{total} correct)\")\n",
    "print(f\"Inference Latency (single sample, median): {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (single sample, 95th percentile): {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "print(f\"Inference Latency (single sample, 99th percentile): {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "print(f\"Inference Throughput (single sample): {100/np.sum(latencies):.2f} FPS\")\n",
    "print(f\"Batch Throughput: {batch_fps:.2f} FPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- \n",
    "\n",
    "compute_gigaio \n",
    "\n",
    "  Model name:             AMD EPYC 7763 64-Core Processor\n",
    "    CPU family:           25\n",
    "    Model:                1\n",
    "    Thread(s) per core:   2\n",
    "    Core(s) per socket:   64\n",
    "\n",
    "-->\n",
    "<!-- summary for mobilenet model\n",
    "\n",
    "Model Size on Disk: 9.23 MB\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 60.16 ms\n",
    "Inference Latency (single sample, 95th percentile): 77.22 ms\n",
    "Inference Latency (single sample, 99th percentile): 77.37 ms\n",
    "Inference Throughput (single sample): 15.82 FPS\n",
    "Batch Throughput: 83.66 FPS\n",
    "\n",
    "\n",
    "Model Size on Disk: 9.23 MB\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 73.97 ms\n",
    "Inference Latency (single sample, 95th percentile): 83.16 ms\n",
    "Inference Latency (single sample, 99th percentile): 83.94 ms\n",
    "Inference Throughput (single sample): 13.34 FPS\n",
    "Batch Throughput: 98.80 FPS\n",
    "\n",
    "-->\n",
    "<!-- summary for mobilenet compiled model\n",
    "\n",
    "Model Size on Disk: 9.23 MB\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 26.92 ms\n",
    "Inference Latency (single sample, 95th percentile): 49.79 ms\n",
    "Inference Latency (single sample, 99th percentile): 64.55 ms\n",
    "Inference Throughput (single sample): 32.35 FPS\n",
    "Batch Throughput: 249.08 FPS\n",
    "\n",
    "Model Size on Disk: 9.23 MB\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 34.14 ms\n",
    "Inference Latency (single sample, 95th percentile): 53.85 ms\n",
    "Inference Latency (single sample, 99th percentile): 60.23 ms\n",
    "Inference Throughput (single sample): 27.39 FPS\n",
    "Batch Throughput: 281.65 FPS\n",
    "\n",
    "-->\n",
    "<!-- \n",
    "\n",
    "(Intel CPU)\n",
    "\n",
    "Model Size on Disk: 9.23 MB\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 12.69 ms\n",
    "Inference Latency (single sample, 95th percentile): 12.83 ms\n",
    "Inference Latency (single sample, 99th percentile): 12.97 ms\n",
    "Inference Throughput (single sample): 78.73 FPS\n",
    "Batch Throughput: 161.27 FPS\n",
    "\n",
    "With compiling\n",
    "\n",
    "Model Size on Disk: 9.23 MB\n",
    "Accuracy: 90.59% (3032/3347 correct)\n",
    "Inference Latency (single sample, median): 8.47 ms\n",
    "Inference Latency (single sample, 95th percentile): 8.58 ms\n",
    "Inference Latency (single sample, 99th percentile): 8.79 ms\n",
    "Inference Throughput (single sample): 117.86 FPS\n",
    "Batch Throughput: 474.67 FPS\n",
    "\n",
    "\n",
    "\n",
    "-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
