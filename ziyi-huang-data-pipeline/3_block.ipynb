{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using block storage\n",
    "\n",
    "Until now, in any experiment we have run on Chameleon, the data in our experiment did not persist beyond the lifetime of our compute. That is, once the VM instance is deleted, any data we may have generated disappears with it. For example, if we were using MLFlow for experiment tracking, when the compute instance that the MLFlow service is running on stops, we would lose all of our tracking history.\n",
    "\n",
    "For a longer-term project, we will of course want to be able to persist data beyond the lifetime of the compute instance. That way, we can provision a compute instance, do some work, delete the compute instance, and then resume later with a *new* compute instance but pick off where we left off with respect to *data*.\n",
    "\n",
    "To enable this, we can create a block storage volume, which can be attached to, detached from, and re-attached to a **VM instance**\\> Data stored on the block storage volume persists until the block storage volume itself is created.\n",
    "\n",
    "After you run this experiment, you will know how to\n",
    "\n",
    "-   create a block storage volume at KVM@TACC,\n",
    "-   attach it to an instance,\n",
    "-   create a filesystem on it and mount it,\n",
    "-   create and use Docker volumes on the block storage volume.\n",
    "-   and re-attach the block storage volume to a new instance after the original compute instance ends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block storage using the Horizon GUI\n",
    "\n",
    "First, let’s try creating a block storage volume from the OpenStack Horizon GUI. Open the GUI for KVM@TACC:\n",
    "\n",
    "-   from the [Chameleon website](https://chameleoncloud.org/hardware/)\n",
    "-   click “Experiment” \\> “KVM@TACC”\n",
    "-   log in if prompted to do so\n",
    "-   check the project drop-down menu near the top left (which shows e.g. “CHI-XXXXXX”), and make sure the correct project is selected.\n",
    "\n",
    "In the menu sidebar on the left side, click on “Volumes” \\> “Volumes” and then, “Create Volume”. You will be prompted to set up your volume step by step using a graphical “wizard”.\n",
    "\n",
    "-   Specify the name as <code>block-persist-<b>projectID</b></code> where in place of <code><b>projectID</b></code> you substitute your team project ID (e.g. `project37` in my case).\n",
    "-   Specify the size as 8 GiB.\n",
    "-   Leave other settings at their defaults, and click “Create Volume”.\n",
    "\n",
    "Next, it’s time to to attach the block storage volume to the compute instance we created earlier. From “Volumes” \\> “Volumes”, next to *your* volume, click the ▼ in the menu on the right and choose “Manage Attachments”. In the “Attach to Instance” menu, choose your compute instance. Then, click “Attach Volume”.\n",
    "\n",
    "Now, the “Volumes” overview page in the Horizon GUI should show something like for your volume:\n",
    "\n",
    "    | Name                | Description | Size | Status | Group | Type     | Attached To                     | Availability Zone | Bootable | Encrypted |\n",
    "    |---------------------|-------------|------|--------|-------|----------|---------------------------------|-------------------|----------|-----------|\n",
    "    | block-persist-netID | -           | 8GiB | In-use | -     | ceph-ssd | /dev/vdb on node-persist-netID  | nova              | No       | No        |\n",
    "\n",
    "On the instance, let’s confirm that we can see the block storage volume. Run\n",
    "\n",
    "``` bash\n",
    "# run on node-persist\n",
    "lsblk\n",
    "```\n",
    "\n",
    "and verify that `vdb` appears in the output.\n",
    "\n",
    "The volume is essentially a raw disk. Before we can use it **for the first time** after creating it, we need to partition the disk, create a filesystem on the partition, and mount it. In subsequent uses, we will only need to mount it.\n",
    "\n",
    "> **Note**: if the volume already had data on it, creating a filesystem on it would erase all its data! This procedure is *only* for the initial setup of a volume, before it has any data on it.\n",
    "\n",
    "First, we create a partition with an `ext4` filesystem, occupying the entire volume:\n",
    "\n",
    "``` bash\n",
    "# run on node-persist\n",
    "sudo parted -s /dev/vdb mklabel gpt\n",
    "sudo parted -s /dev/vdb mkpart primary ext4 0% 100%\n",
    "```\n",
    "\n",
    "Verify that we now have the partition `vdb1` in the output of\n",
    "\n",
    "``` bash\n",
    "# run on node-persist\n",
    "lsblk\n",
    "```\n",
    "\n",
    "Next, we format the partition:\n",
    "\n",
    "``` bash\n",
    "# run on node-persist\n",
    "sudo mkfs.ext4 /dev/vdb1\n",
    "```\n",
    "\n",
    "Finally, we can create a directory in the local filesystem, mount the partition to that directory:\n",
    "\n",
    "``` bash\n",
    "# run on node-persist\n",
    "sudo mkdir -p /mnt/block\n",
    "sudo mount /dev/vdb1 /mnt/block\n",
    "```\n",
    "\n",
    "and change the owner of that directory to the `cc` user:\n",
    "\n",
    "``` bash\n",
    "# run on node-persist\n",
    "sudo chown -R cc /mnt/block\n",
    "sudo chgrp -R cc /mnt/block\n",
    "```\n",
    "\n",
    "Run\n",
    "\n",
    "``` bash\n",
    "# run on node-persist\n",
    "df -h\n",
    "```\n",
    "\n",
    "and verify that the output includes a line with `/dev/vdb1` mounted on `/mnt/block`:\n",
    "\n",
    "    Filesystem      Size  Used Avail Use% Mounted on\n",
    "    /dev/vdb1       7.8G   24K  7.4G   1% /mnt/block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Docker volumes on persistent storage\n",
    "\n",
    "Now that we have a block storage volume attached to our VM instance, let’s see how persistent storage can be useful.\n",
    "\n",
    "Suppose we are going to train some ML models. We will use MLFlow for experiment tracking. However, we won’t necessarily be running MLFlow *all* the time. We will probably have to bring our “platform” VM(s) down and up as we iterate on our platform design. We don’t want to lose all past experiment logs and models every time we bring the VMs down.\n",
    "\n",
    "MLFLow uses two types of data systems: a relational database (Postgresql) for experiments, metrics, and parameters; and for unstructured data like artifacts and saved models, a MinIO object store. (We could hypothetically ask MinIO to use Chameleon’s object store instead of running our own MinIO, but since we have already set it up for MinIO, we’ll stick to that.)\n",
    "\n",
    "We can use a persistent block storage backend for both types of data storage to make sure that experiment logs and models persist even when the VM instance hosting MLFlow is not running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to use Docker Compose to bring up a set of services on the VM instance:\n",
    "\n",
    "-   an MLFlow server.\n",
    "-   a Postgresql database with persistent storage: the host directory `/mnt/block/postgres_data`, which is on the block storage volume, is going to be mounted to `/var/lib/postgresql/data` inside the container.\n",
    "-   a MinIO object store with persistent storage: the host directory `/mnt/block/minio_data`, which is on the block storage volume, is going to be mounted to `/data` inside the container.\n",
    "-   and a Jupyter server. As before, we pass the object store mount to the Jupyter server, so that it can also access the Food11 dataset in the object store.\n",
    "\n",
    "To bring up these services, run\n",
    "\n",
    "``` bash\n",
    "# run on node-persist\n",
    "HOST_IP=$(curl --silent http://169.254.169.254/latest/meta-data/public-ipv4 ) docker compose -f ~/MLSysEngOps-NYU25Spring/ziyi-huang-data-pipeline/docker/docker-compose-block.yaml up -d\n",
    "```\n",
    "\n",
    "(we need to define `HOST_IP` so that we can set the MLFLow tracking URI in the Jupyter environment.)\n",
    "\n",
    "Run\n",
    "\n",
    "``` bash\n",
    "# run on node-persist\n",
    "docker logs jupyter\n",
    "```\n",
    "\n",
    "and look for a line like\n",
    "\n",
    "    http://127.0.0.1:8888/lab?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "\n",
    "Paste this into a browser tab, but in place of 127.0.0.1, substitute the floating IP assigned to your instance, to open the Jupyter notebook interface that is running on your compute instance. In the “work” directory, find and open “demo.ipynb”.\n",
    "\n",
    "Also open the MLFlow service web UI: it is at\n",
    "\n",
    "    http://A.B.C.D:8000\n",
    "\n",
    "where in place of `A.B.C.D`, you substitute the floating IP assigned to your instance.\n",
    "\n",
    "Let’s add some MLFlow tracking to our “demo.ipynb” notebook. (There’s no model training in that notebook - it’s just an evaluation - but it works for demo purposes!) At the end, add a cell:\n",
    "\n",
    "``` python\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "mlflow.set_experiment(\"movielens-ssept-eval\")\n",
    "avg_score = np.mean(results)\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_metric(\"avg_score\", avg_score)\n",
    "    mlflow.pytorch.log_model(model, \"ssept-model\")\n",
    "```\n",
    "\n",
    "and run the notebook.\n",
    "\n",
    "Confirm in the MLFlow UI that both items are logged:\n",
    "\n",
    "-   the evaluation accuracy is logged as a metric, which will be stored in the Postgresql relational database\n",
    "-   the model is logged as an artifact, which will be stored in a MinIO bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s confirm that the MLFlow data persists beyond the lifetime of the compute instance! We will now delete the compute instance.\n",
    "\n",
    "The following cells run in the **Chameleon** Jupyter environment (not in the Jupyter environment that you are hosting on your compute instance!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in Chameleon Jupyter environment\n",
    "from chi import context, server\n",
    "import chi\n",
    "import os\n",
    "\n",
    "context.version = \"1.0\" \n",
    "context.choose_project()  # Select the correct project\n",
    "context.choose_site(default=\"KVM@TACC\")\n",
    "username = os.getenv('USER') # exp resources will have this suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in Chameleon Jupyter environment\n",
    "# delete the old server instance!\n",
    "s_old = server.get_server(f\"node-persist-project37\")\n",
    "s_old.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in Chameleon Jupyter environment\n",
    "s = server.Server(\n",
    "    f\"node-persist-project37\", \n",
    "    image_name=\"CC-Ubuntu24.04\",\n",
    "    flavor_name=\"m1.large\"\n",
    ")\n",
    "s.submit(idempotent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in Chameleon Jupyter environment\n",
    "s.associate_floating_ip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in Chameleon Jupyter environment\n",
    "s.refresh()\n",
    "s.check_connectivity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in Chameleon Jupyter environment\n",
    "s.refresh()\n",
    "s.show(type=\"widget\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in Chameleon Jupyter environment\n",
    "security_groups = [\n",
    "  {'name': \"allow-ssh\", 'port': 22, 'description': \"Enable SSH traffic on TCP port 22\"},\n",
    "  {'name': \"allow-8888\", 'port': 8888, 'description': \"Enable TCP port 8888 (used by Jupyter)\"},\n",
    "  {'name': \"allow-8000\", 'port': 8000, 'description': \"Enable TCP port 8000 (used by MLFlow)\"},\n",
    "  {'name': \"allow-9000\", 'port': 9000, 'description': \"Enable TCP port 9000 (used by MinIO API)\"},\n",
    "  {'name': \"allow-9001\", 'port': 9001, 'description': \"Enable TCP port 9001 (used by MinIO Web UI)\"}\n",
    "]\n",
    "\n",
    "os_conn = chi.clients.connection()\n",
    "nova_server = chi.nova().servers.get(s.id)\n",
    "\n",
    "for sg in security_groups:\n",
    "  nova_server.add_security_group(sg['name'])\n",
    "\n",
    "print(f\"updated security groups: {[group.name for group in nova_server.list_security_group()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in Chameleon Jupyter environment\n",
    "s.execute(\"git clone https://github.com/teaching-on-testbeds/data-persist-chi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in Chameleon Jupyter environment\n",
    "s.execute(\"curl -sSL https://get.docker.com/ | sudo sh\")\n",
    "s.execute(\"sudo groupadd -f docker; sudo usermod -aG docker $USER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell will attach the block storage volume named “block-persist-**netID**” to your compute instance - edit it to substitute your *own* net ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in Chameleon Jupyter environment\n",
    "cinder_client = chi.clients.cinder()\n",
    "volume = [v for v in cinder_client.volumes.list() if v.name=='block-persist-project37'][0] # Substitute your team ID\n",
    "\n",
    "volume_manager = chi.nova().volumes\n",
    "volume_manager.create_server_volume(server_id = s.id, volume_id = volume.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can confirm in the Horizon GUI that your block storage volume is now attached to the new compute instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s confirm that data we put on the block storage volume earlier, is now available on the new compute instance.\n",
    "\n",
    "Connect to the new instance over SSH. Mount the block storage volume:\n",
    "\n",
    "``` bash\n",
    "# run on node-persist\n",
    "sudo mkdir -p /mnt/block\n",
    "sudo mount /dev/vdb1 /mnt/block\n",
    "```\n",
    "\n",
    "and confirm that it is not empty:\n",
    "\n",
    "``` bash\n",
    "# run on node-persist\n",
    "ls /mnt/block\n",
    "```\n",
    "\n",
    "for example, you can see previously logged artifacts saved by MinIO:\n",
    "\n",
    "``` bash\n",
    "# run on node-persist\n",
    "ls /mnt/block/minio_data/mlflow-artifacts/1/\n",
    "```\n",
    "\n",
    "Use Docker compose to bring up the services again:\n",
    "\n",
    "``` bash\n",
    "# run on node-persist\n",
    "HOST_IP=$(curl --silent http://169.254.169.254/latest/meta-data/public-ipv4 ) docker compose -f ~/data-persist-chi/docker/docker-compose-block.yaml up -d\n",
    "```\n",
    "\n",
    "In your browser, open the MLFlow service web UI at\n",
    "\n",
    "    http://A.B.C.D:8000\n",
    "\n",
    "where in place of `A.B.C.D`, you substitute the floating IP assigned to your instance. Verify that the experiment runs logged by the previous compute instance are persisted to the new MLFlow instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This MLFlow demo is just an example - the same principle applies to any other platform service we might use. Services like Prometheus that run directly on a VM can use an attached block storage volume. Services like Ray, which run on bare metal for GPU training, can use a MinIO storage backend that is hosted on a VM, and uses an attached block storage volume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference: creating block volumes storage using Python\n",
    "\n",
    "We created our block storage volume using the Horizon GUI. However it is also worthwhile to learn how to create and manage block storage volumes directly in Python, if you are automating infrastructure setup using a Python notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In OpenStack, the Cinder service provides block storage volumes. We can access the already-configured (authenticated) Cinder client from `python-chi` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in Chameleon Jupyter environment\n",
    "# get the Cinder Python client configured by python-chi\n",
    "cinder_client = chi.clients.cinder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in Chameleon Jupyter environment\n",
    "# list current volumes\n",
    "cinder_client.volumes.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the Cinder client to create a *new* block storage volume:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in Chameleon Jupyter environment\n",
    "# create a volume, specifying name and size in GiB\n",
    "volume = cinder_client.volumes.create(name=f\"block-persist-python-project37\", size=8)\n",
    "volume._info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can attach the volume to a compute instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in Chameleon Jupyter environment\n",
    "server_id = chi.server.get_server(f\"node-persist-project37\").id\n",
    "volume_manager = chi.nova().volumes\n",
    "volume_manager.create_server_volume(server_id = s.id, volume_id = volume.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or detach the volume from a compute instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in Chameleon Jupyter environment\n",
    "volume_manager.delete_server_volume(server_id = s.id, volume_id = volume.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, to completely delete a volume (loses all the data!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in Chameleon Jupyter environment\n",
    "cinder_client.volumes.delete(volume = volume)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
