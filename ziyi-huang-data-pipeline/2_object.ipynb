{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using object storage\n",
    "\n",
    "Until now, in any experiment we have run on Chameleon, we had to re-download large training sets each time we launched a new compute instance to work on that data. For example, in our “GourmetGram” use case, we had to re-download the Project37 movielens dataset each time we brought up a compute instance to train or evaluate a model on that data.\n",
    "\n",
    "For a longer-term project, we will want to persist large data sets beyond the lifetime of the compute instance. That way, we can download a very large data set *once* and then re-use it many times with different compute instances, without having to keep a compute instance “alive” all the time, or re-download the data. We will use the object storage service in Chameleon to enable this.\n",
    "\n",
    "Of the various types of storage available in a cloud computing environment (object, block, file), object storage is the most appropriate for large training data sets. Object storage is cheap, and optimized for storing and retrieving large volumes of data, where the data is not modified frequently. (In object storage, there is no in-place modification of objects - only replacement - so it is not the best solution for files that are frequently modified.)\n",
    "\n",
    "After you run this experiment, you will know how to:\n",
    "\n",
    "-   create an object store container at CHI@TACC\n",
    "-   copy objects to it,\n",
    "-   and mount it as a filesystem in a compute instance.\n",
    "\n",
    "The object storage service is available at CHI@TACC or CHI@UC. In this tutorial, we will use CHI@TACC. The CHI@TACC object store can be accessed from a KVM@TACC VM instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object storage using the Horizon GUI\n",
    "\n",
    "First, let’s try creating an object storage container from the OpenStack Horizon GUI.\n",
    "\n",
    "Open the GUI for CHI@TACC:\n",
    "\n",
    "-   from the [Chameleon website](https://chameleoncloud.org/hardware/)\n",
    "-   click “Experiment” \\> “CHI@TACC”\n",
    "-   log in if prompted to do so\n",
    "-   check the project drop-down menu near the top left (which shows e.g. “CHI-XXXXXX”), and make sure the correct project is selected.\n",
    "\n",
    "In the menu sidebar on the left side, click on “Object Store” \\> “Containers” and then, “Create Container”. You will be prompted to set up your container step by step using a graphical “wizard”.\n",
    "\n",
    "-   Specify the name as <code>object-persist-<b>teamID</b></code> where in place of <code><b>teamID</b></code> you substitute your team ID (e.g. `project37` in our team case).\n",
    "-   Leave other settings at their defaults, and click “Submit”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `rclone` and authenticate to object store from a compute instance\n",
    "\n",
    "We will want to connect to this object store from the compute instance we configured earlier, and copy some data to it!\n",
    "\n",
    "For *write* access to the object store from the compute instance, we will need to authenticate with valid OpenStack credentials. To support this, we will create an *application credential*, which consists of an ID and a secret that allows a script or application to authenticate to the service.\n",
    "\n",
    "An application credential is a good way for something like a data pipeline to authenticate, since it can be used non-interactively, and can be revoked easily in case it is compromised without affecting the entire user account.\n",
    "\n",
    "In the menu sidebar on the left side of the Horizon GUI, click “Identity” \\> “Application Credentials”. Then, click “Create Application Credential”.\n",
    "\n",
    "-   In the “Name”, field, use “data-persist”.\n",
    "-   Set the “Expiration” date to the end date of the current semester. (Note that this will be in UTC time, not your local time zone.) This ensures that if your credential is leaked (e.g. you accidentially push it to a public Github repository), the damage is mitigated.\n",
    "-   Click “Create Application Credential”.\n",
    "-   Copy the “ID” and “Secret” displayed in the dialog, and save them in a safe place. You will not be able to view the secret again from the Horizon GUI. Then, click “Download openrc file” to have another copy of the secret.\n",
    "\n",
    "Now that we have an application credential, we can use it to allow an application to authenticate to the Chameleon object store service. There are several applications and utilities for working with OpenStack’s Swift object store service; we will use one called [`rclone`](https://github.com/rclone/rclone).\n",
    "\n",
    "On the compute instance, install `rclone`:\n",
    "\n",
    "``` bash\n",
    "# run on node-persist\n",
    "curl https://rclone.org/install.sh | sudo bash\n",
    "```\n",
    "\n",
    "We also need to modify the configuration file for FUSE (**F**ilesystem in **USE**rspace: the interface that allows user space applications to mount virtual filesystems), so that object store containers mounted by our user will be availabe to others, including Docker containers:\n",
    "\n",
    "``` bash\n",
    "# run on node-persist\n",
    "# this line makes sure user_allow_other is un-commented in /etc/fuse.conf\n",
    "sudo sed -i '/^#user_allow_other/s/^#//' /etc/fuse.conf\n",
    "```\n",
    "\n",
    "Next, create a configuration file for `rclone` with the ID and secret from the application credential you just generated:\n",
    "\n",
    "``` bash\n",
    "# run on node-persist\n",
    "mkdir -p ~/.config/rclone\n",
    "nano  ~/.config/rclone/rclone.conf\n",
    "```\n",
    "\n",
    "Paste the following into the config file, but substitute your own application credential ID and secret.\n",
    "\n",
    "You will also need to substitute your own user ID. You can find it using “Identity” \\> “Users” in the Horizon GUI; it is an alphanumeric string (*not* the human-readable user name).\n",
    "\n",
    "    [chi_tacc]\n",
    "    type = swift\n",
    "    user_id = YOUR_USER_ID\n",
    "    application_credential_id = APP_CRED_ID\n",
    "    application_credential_secret = APP_CRED_SECRET\n",
    "    auth = https://chi.tacc.chameleoncloud.org:5000/v3\n",
    "    region = CHI@TACC\n",
    "\n",
    "Use Ctrl+O and Enter to save the file, and Ctrl+X to exit `nano`.\n",
    "\n",
    "To test it, run\n",
    "\n",
    "``` bash\n",
    "# run on node-persist\n",
    "rclone lsd chi_tacc:\n",
    "```\n",
    "\n",
    "and verify that you see your container listed. This confirms that `rclone` can authenticate to the object store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Retrieve code on the node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "``` bash\n",
    "# run on node-persist\n",
    "git clone https://github.com/zuchuandatou/MLSysEngOps-NYU25Spring.git\n",
    "cd MLSysEngOps-NYU25Spring\n",
    "git checkout ziyi-huang/data-pipeline\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a pipeline to load training data into the object store\n",
    "\n",
    "Next, we will prepare a simple ETL pipeline to get the Project37 movielens dataset into the object store. It will:\n",
    "\n",
    "-   extract the data into a staging area (local filesystem on the instance)\n",
    "-   transform the data, organizing it into directories by class as required by PyTorch\n",
    "-   and then load the data into the object store\n",
    "\n",
    "We are going to define the pipeline stages inside a Docker compose file. All of the services in the container will share a common `project37` volume. Then, we have:\n",
    "\n",
    "1.  A service to extract the Project37 movielens data from the Internet. This service runs a Python container image, downloads the dataset, and unzips it.\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "    extract-data:\n",
    "    container_name: etl_extract_data\n",
    "    image: python:3.11\n",
    "    user: root\n",
    "    volumes:\n",
    "      - project37:/data\n",
    "    working_dir: /data\n",
    "    command:\n",
    "      - bash\n",
    "      - -c\n",
    "      - |\n",
    "        set -e\n",
    "\n",
    "        echo \"Resetting dataset directory...\"\n",
    "        rm -rf /data/Project-37\n",
    "        mkdir -p /data/Project-37\n",
    "        cd /data/Project-37\n",
    "\n",
    "        echo \"Downloading MovieLens 192M zip...\"\n",
    "        curl -L \"https://nyu.box.com/shared/static/5r8m3rvjfejcip7nqurf9jbpi5rw5ri1?dl=1\" -o ml-192m.zip\n",
    "\n",
    "        echo \"Unzipping dataset...\"\n",
    "        unzip -q ml-192m.zip\n",
    "        rm -f ml-192m.zip\n",
    "\n",
    "        echo \"Listing contents of /data after extract stage:\"\n",
    "        find /data/Project-37 -mindepth 1 -maxdepth 1 -type d\n",
    "\n",
    "1.  A service that runs a Python container image, and uses a Python script to organize the data into directories according to class label.\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "    transform-data:\n",
    "    container_name: etl_transform_data\n",
    "    image: python:3.11\n",
    "    volumes:\n",
    "      - project37:/data\n",
    "    working_dir: /data/Project-37\n",
    "    command:\n",
    "      - bash\n",
    "      - -c\n",
    "      - |\n",
    "        set -e\n",
    "\n",
    "        echo \"Installing required Python packages...\"\n",
    "        pip install --no-cache-dir pandas scikit-learn\n",
    "\n",
    "        echo \"Transforming MovieLens 192M dataset...\"\n",
    "        python3 -c '\n",
    "        import os\n",
    "        import pandas as pd\n",
    "        import random\n",
    "\n",
    "        base_dir = \"/data/Project-37\"\n",
    "        csv_path = os.path.join(base_dir, \"ml-192m\", \"ratings.csv\")\n",
    "        data_name = \"movielens_192m\"\n",
    "        output_dir = os.path.join(base_dir, \"raw\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        train_path = os.path.join(base_dir, \"training\", f\"{data_name}_train.txt\")\n",
    "        val_path = os.path.join(base_dir, \"validation\", f\"{data_name}_val.txt\")\n",
    "        eval_path = os.path.join(base_dir, \"evaluation\", f\"{data_name}_eval.txt\")\n",
    "\n",
    "        for p in [train_path, val_path, eval_path]:\n",
    "          os.makedirs(os.path.dirname(p), exist_ok=True)\n",
    "\n",
    "        # Pass 1: Build ID maps\n",
    "        print(\"Pass 1: Building user/item ID maps...\")\n",
    "        user_set = set()\n",
    "        item_set = set()\n",
    "        for chunk in pd.read_csv(csv_path, chunksize=1_000_000):\n",
    "          user_set.update(chunk[\"userId\"].unique())\n",
    "          item_set.update(chunk[\"movieId\"].unique())\n",
    "\n",
    "        user_map = {u: i + 1 for i, u in enumerate(sorted(user_set))}\n",
    "        item_map = {m: i + 1 for i, m in enumerate(sorted(item_set))}\n",
    "\n",
    "        # Pass 2: Remap + assign each row to split immediately\n",
    "        print(\"Pass 2: Remapping + splitting rows to output...\")\n",
    "        split_probs = [0.7, 0.15, 0.15]\n",
    "\n",
    "        with open(train_path, \"w\") as f_train, open(val_path, \"w\") as f_val, open(eval_path, \"w\") as f_eval:\n",
    "          for chunk in pd.read_csv(csv_path, chunksize=1_000_000):\n",
    "            chunk[\"user_id\"] = chunk[\"userId\"].map(user_map)\n",
    "            chunk[\"item_id\"] = chunk[\"movieId\"].map(item_map)\n",
    "            for _, row in chunk.iterrows():\n",
    "              line = f\"{int(row.user_id)}\\t{int(row.item_id)}\\n\"\n",
    "              r = random.random()\n",
    "              if r < split_probs[0]:\n",
    "                f_train.write(line)\n",
    "              elif r < split_probs[0] + split_probs[1]:\n",
    "                f_val.write(line)\n",
    "              else:\n",
    "                f_eval.write(line)\n",
    "\n",
    "        print(\"Stats:\")\n",
    "        print(f\"Total users: { len(user_map) }, items: { len(item_map) }\")\n",
    "        print(\"Transform complete.\")\n",
    "        '\n",
    "\n",
    "        echo \"Listing contents after transform:\"\n",
    "        find /data/Project-37 -mindepth 1 -maxdepth 1 -type d\n",
    "\n",
    "\n",
    "1.  And finally, a service that uses `rclone copy` to load the organized data into the object store. Note that we pass some arguments to `rclone copy` to increase the parallelism, so that the data is loaded more quicly. Also note that since the name of the container includes your **team ID**, we have specified it using an environment variable that must be set before this stage can run.\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "  load-data:\n",
    "    container_name: etl_load_data\n",
    "    image: rclone/rclone:latest\n",
    "    volumes:\n",
    "      - project37:/data\n",
    "      - ~/.config/rclone/rclone.conf:/root/.config/rclone/rclone.conf:ro\n",
    "    entrypoint: /bin/sh\n",
    "    command:\n",
    "      - -c\n",
    "      - |\n",
    "        if [ -z \"$RCLONE_CONTAINER\" ]; then\n",
    "          echo \"ERROR: RCLONE_CONTAINER is not set\"\n",
    "          exit 1\n",
    "        fi\n",
    "        \n",
    "        echo \"Cleaning up existing contents of container...\"\n",
    "        rclone delete chi_tacc:$RCLONE_CONTAINER --rmdirs || true\n",
    "        \n",
    "        echo \"Uploading data from /data/Project-37...\"\n",
    "        rclone copy /data/Project-37 chi_tacc:$RCLONE_CONTAINER \\\n",
    "          --progress \\\n",
    "          --transfers=32 \\\n",
    "          --checkers=16 \\\n",
    "          --multi-thread-streams=4 \\\n",
    "          --fast-list\n",
    "        \n",
    "        echo \"Listing directories in container after load stage:\"\n",
    "        rclone lsd chi_tacc:$RCLONE_CONTAINER\n",
    "\n",
    "These services are defined in `~/MLSysEngOps-NYU25Spring/ziyi-huang-data-pipeline/docker/docker-compose-etl.yaml`.\n",
    "\n",
    "Now, we can run the stages using Docker. (If we had a workflow orchestrator, we could use it to run the pipeline stages - but we don’t really need orchestration at this point.)\n",
    "\n",
    "``` bash\n",
    "# run on node-persist\n",
    "cd ziyi-huang-data-pipeline/docker\n",
    "```\n",
    "\n",
    "``` bash\n",
    "# run on node-persist\n",
    "docker compose -f docker-compose-etl.yaml run extract-data\n",
    "```\n",
    "\n",
    "``` bash\n",
    "# run on node-persist\n",
    "docker compose -f docker-compose-etl.yaml run transform-data\n",
    "```\n",
    "\n",
    "For the last stage, the container name is not specified in the Docker compose YAML (since it has your **team ID** in it) - so we have to pass it as an environment variable first. Substitute your own **team ID** in the line below:\n",
    "\n",
    "``` bash\n",
    "# run on node-persist\n",
    "export RCLONE_CONTAINER=object-persist-project37\n",
    "docker compose -f docker-compose-etl.yaml run load-data\n",
    "```\n",
    "\n",
    "Now our training data is loaded into the object store and ready to use for training! We can clean up the Docker volume used as the temporary staging area:\n",
    "\n",
    "``` bash\n",
    "# run on node-persist\n",
    "docker container prune -f\n",
    "docker volume rm project37-etl_project37\n",
    "```\n",
    "\n",
    "In the Horizon GUI, note that we can browse the object store and download any file from it. This container is independent of any compute instance - it will persist, and its data is still saved, even if we have no active compute instance. (In fact, we *already* have no active compute instance on CHI@TACC.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mount an object store to local file system\n",
    "\n",
    "Now that our data is safely inside the object store, we can use it anywhere - on a VM, on a bare metal site, on multiple compute instances at once, even outside of Chameleon - to train or evaluate a model. We would not have to repeat the ETL pipeline each time we want to use the data.\n",
    "\n",
    "If we were working on a brand-new compute instance, we would need to download `rclone` and create the `rclone` configuration file at `~/.config/rclone.conf`, as we have above. Since we already done these steps in order to load data into the object store, we don’t need to repeat them.\n",
    "\n",
    "The next step is to create a mount point for the data in the local filesystem:\n",
    "\n",
    "``` bash\n",
    "# run on node-persist\n",
    "sudo mkdir -p /mnt/object\n",
    "sudo chown -R cc /mnt/object\n",
    "sudo chgrp -R cc /mnt/object\n",
    "```\n",
    "\n",
    "Now finally, we can use `rclone mount` to mount the object store at the mount point (substituting your own **team ID** in the command below).\n",
    "\n",
    "``` bash\n",
    "# run on node-persist\n",
    "rclone mount chi_tacc:object-persist-teamID /mnt/object --read-only --allow-other --daemon\n",
    "```\n",
    "\n",
    "Here,\n",
    "\n",
    "-   `chi_tacc` tells `rclone` which section of its configuration file to use for authentication information\n",
    "-   `object-persist-netID` tells it what object store container to mount\n",
    "-   `/mnt/object` says where to mount it\n",
    "\n",
    "Since we only intend to read the data, we can mount it in read-only mode and it will be slightly faster; and we are also protected from accidental writes. We also specified `--allow-other` so that we can use the mount from Docker, and `--daemon` means the `rclone` process will be started in the background.\n",
    "\n",
    "Run\n",
    "\n",
    "``` bash\n",
    "# run on node-persist\n",
    "ls /mnt/object\n",
    "```\n",
    "\n",
    "and confirm that we can now see the Project37 movielens data directories (`ml-32m`, `staging`, `evaluation`, `training`, `validation`) there.\n",
    "\n",
    "\n",
    "# TODO:\n",
    "Now, we can start a Docker container with access to that virtual “filesystem”, by passing that directory as a bind mount. Note that to mount a directory that is actually a FUSE filesystem inside a Docker container, we have to pass it using a slightly different `--mount` syntax, instead of the `-v` that we had used in previous examples.\n",
    "\n",
    "``` bash\n",
    "# run on node-persist\n",
    "docker run -d --rm \\\n",
    "  -p 8888:8888 \\\n",
    "  --shm-size 8G \\\n",
    "  -e MOVIELENS_DATA_DIR=/mnt/Project-37 \\\n",
    "  -v ~/MLSysEngOps-NYU25Spring:/home/jovyan/work/ \\\n",
    "  --mount type=bind,source=/mnt/object,target=/mnt/Project-37,readonly \\\n",
    "  --name jupyter \\\n",
    "  quay.io/jupyter/pytorch-notebook:latest\n",
    "\n",
    "```\n",
    "\n",
    "Run\n",
    "\n",
    "``` bash\n",
    "# run on node-persist\n",
    "docker logs jupyter\n",
    "```\n",
    "\n",
    "and look for a line like\n",
    "\n",
    "    http://127.0.0.1:8888/lab?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "\n",
    "Paste this into a browser tab, but in place of 127.0.0.1, substitute the floating IP assigned to your instance, to open the Jupyter notebook interface that is running on your compute instance.\n",
    "\n",
    "Then, find the `demo.ipynb` notebook. This notebook evaluates the `food11.pth` model on the evaluation set, which is **streamed from the object store**.\n",
    "\n",
    "To validate this, on the host, run\n",
    "\n",
    "``` bash\n",
    "# run on node-persist\n",
    "sudo apt update\n",
    "sudo apt -y install nload\n",
    "nload ens3\n",
    "```\n",
    "\n",
    "to monitor the load on the network. Run the `demo.ipynb` notebook inside the Jupyter instance running on “node-persist”, which also watching the `nload` output.\n",
    "\n",
    "Note the incoming data volume, which should be on the order of Mbits/second when a batch is being loaded.\n",
    "\n",
    "Close the Jupyter container tab in your browser, and then stop the container with\n",
    "\n",
    "``` bash\n",
    "# run on node-persist\n",
    "docker stop jupyter\n",
    "```\n",
    "\n",
    "since we will bring up a different Jupyter instance in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un-mount an object store\n",
    "\n",
    "We’ll keep working with this object store in the next part, so you do not have to un-mount it now. But generally speaking to stop `rclone` running and un-mount the object store, you would run\n",
    "\n",
    "    fusermount -u /mnt/object\n",
    "\n",
    "where you specify the path of the mount point."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
