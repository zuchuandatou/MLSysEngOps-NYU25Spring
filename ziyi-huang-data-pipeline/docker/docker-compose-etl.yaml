name: project37-etl

volumes:
  project37:

services:
  # To run it: docker compose -f docker-compose-etl.yaml run extract-data
  extract-data:
    container_name: etl_extract_data
    image: python:3.11
    user: root
    volumes:
      - project37:/data
    working_dir: /data
    command:
      - bash
      - -c
      - |
        set -e

        echo "Resetting dataset directory..."
        rm -rf /data/Project-37
        mkdir -p /data/Project-37
        cd /data/Project-37

        echo "Downloading MovieLens 32M zip..."
        curl -L https://files.grouplens.org/datasets/movielens/ml-32m.zip -o ml-32m.zip

        echo "Unzipping dataset..."
        unzip -q ml-32m.zip
        rm -f ml-32m.zip

        echo "Listing contents of /data after extract stage:"
        find /data/Project-37 | head -30
        

  # To run it: docker compose -f docker-compose-etl.yaml run transform-data
  transform-data:
    container_name: etl_transform_data
    image: python:3.11
    volumes:
      - project37:/data
    working_dir: /data/Project-37
    command:
      - bash
      - -c
      - |
        set -e

        echo "Installing required Python packages..."
        pip install --no-cache-dir pandas scikit-learn

        echo "Transforming MovieLens 32M dataset..."
        python3 -c '
        import os
        import pandas as pd
        from sklearn.model_selection import train_test_split

        base_dir = "/data/Project-37"
        data_name = "movielens_32m"
        output_dir = os.path.join(base_dir, "staging")
        os.makedirs(output_dir, exist_ok=True)

        csv_path = os.path.join(base_dir, "ml-32m", "ratings.csv")
        print("Loading ratings.csv...")
        names = ["user_id", "item_id", "rating", "timestamp"]
        data = pd.read_csv(csv_path, names=names, header=0)

        print("Remapping user and item IDs...")
        user_set = data["user_id"].unique()
        item_set = data["item_id"].unique()
        user_map = {user: i + 1 for i, user in enumerate(user_set)}
        item_map = {item: i + 1 for i, item in enumerate(item_set)}
        data["user_id"] = data["user_id"].map(user_map)
        data["item_id"] = data["item_id"].map(item_map)

        print("Sorting by user_id and timestamp...")
        data = data.sort_values(by=["user_id", "timestamp"])

        num_users = data["user_id"].nunique()
        num_items = data["item_id"].nunique()
        sparsity = 1 - len(data) / (num_users * num_items)
        print(f"number of users: {num_users}, number of items: {num_items}")
        print(f"matrix sparsity: {sparsity:f}")
        print(data.head(5))

        df = data.drop(columns=["timestamp", "rating"])

        print("Splitting into train/val/eval sets...")
        train, temp = train_test_split(df, test_size=0.3, random_state=42)
        val, eval_ = train_test_split(temp, test_size=0.5, random_state=42)

        for part in ["training", "validation", "evaluation"]:
            os.makedirs(os.path.join(base_dir, part), exist_ok=True)

        df.to_csv(os.path.join(output_dir, f"{data_name}.txt"), sep="\t", header=False, index=False)
        train.to_csv(os.path.join(base_dir, "training", f"{data_name}_train.txt"), sep="\t", header=False, index=False)
        val.to_csv(os.path.join(base_dir, "validation", f"{data_name}_val.txt"), sep="\t", header=False, index=False)
        eval_.to_csv(os.path.join(base_dir, "evaluation", f"{data_name}_eval.txt"), sep="\t", header=False, index=False)

        print("Transform complete.")
        '

        echo "Listing contents after transform:"
        find /data/Project-37 -mindepth 1 -maxdepth 1 -type d

  # To run it: docker compose -f docker-compose-etl.yaml run load-data
  load-data:
    container_name: etl_load_data
    image: rclone/rclone:latest
    volumes:
      - project37:/data
      - ~/.config/rclone/rclone.conf:/root/.config/rclone/rclone.conf:ro
    entrypoint: /bin/sh
    command:
      - -c
      - |
        if [ -z "$RCLONE_CONTAINER" ]; then
          echo "ERROR: RCLONE_CONTAINER is not set"
          exit 1
        fi
        
        echo "Cleaning up existing contents of container..."
        rclone delete chi_tacc:$RCLONE_CONTAINER --rmdirs || true
        
        echo "Uploading data from /data/Project-37..."
        rclone copy /data/Project-37 chi_tacc:$RCLONE_CONTAINER \
          --progress \
          --transfers=32 \
          --checkers=16 \
          --multi-thread-streams=4 \
          --fast-list
        
        echo "Listing directories in container after load stage:"
        rclone lsd chi_tacc:$RCLONE_CONTAINER