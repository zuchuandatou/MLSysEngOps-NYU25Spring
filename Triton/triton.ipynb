{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Triton Inference Server\n",
    "\n",
    "### Deploying SSEPT Model with Triton Inference Server\n",
    "\n",
    "We use NVIDIA's Triton Inference Server to serve our SSEPT model in a scalable and optimized way. This includes:\n",
    "\n",
    "- Deploying via Triton's Python backend\n",
    "- Enabling dynamic batching\n",
    "- Running on GPU (or CPU)\n",
    "- Integrating with Flask UI\n",
    "- Running benchmarks from a Jupyter container\n",
    "\n",
    "---\n",
    "\n",
    "#### Triton Model Structure\n",
    "\n",
    "Directory layout:\n",
    "```\n",
    "serve-system-chi/models/\n",
    "└── recommender_model/\n",
    "├── config.pbtxt\n",
    "├── 1/\n",
    "│ ├── SSE_PT10kemb.onnx or .pth\n",
    "│ └── model.py\n",
    "```\n",
    "\n",
    "\n",
    "#### config.pbtxt (example)\n",
    "\n",
    "```protobuf\n",
    "name: \"recommender_model\"\n",
    "backend: \"python\"\n",
    "max_batch_size: 16\n",
    "input [\n",
    "  { name: \"USER_ID\" data_type: TYPE_INT64 dims: [1] },\n",
    "  { name: \"SEQ\" data_type: TYPE_INT64 dims: [50] }\n",
    "]\n",
    "output [\n",
    "  { name: \"TOP_K\" data_type: TYPE_INT64 dims: [5] }\n",
    "]\n",
    "instance_group [\n",
    "  { count: 1 kind: KIND_GPU gpus: [ 0 ] }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model.py (Python backend interface)\n",
    "\n",
    "implement a TritonPythonModel class:\n",
    "\n",
    "def initialize(self, args):\n",
    "    # Load ONNX or PyTorch model\n",
    "    self.model = load_model(...)\n",
    "    self.device = select_device_from(args)\n",
    "\n",
    "def execute(self, requests):\n",
    "    # For each request in batch:\n",
    "    # - Extract user_id and sequence\n",
    "    # - Run inference\n",
    "    # - Return top_k as output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Triton + Flask + Jupyter\n",
    "\n",
    "docker compose -f ~/serve-system-chi/docker/docker-compose-triton.yaml up -d\n",
    "\n",
    "This launches:\n",
    " triton_server: GPU inference backend\n",
    " flask_ui: sends JSON requests to Triton\n",
    " jupyter: used to run benchmarks\n",
    "\n",
    "Verify Triton logs:\n",
    "    docker logs triton_server -f\n",
    "    \n",
    "Expected output:\n",
    "\n",
    "  | recommender_model | 1 | READY |\n",
    "  Started GRPCInferenceService at 0.0.0.0:8001\n",
    "  Started HTTPService at 0.0.0.0:8000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access UI and Jupyter\n",
    "\n",
    "Flask Inference UI:\n",
    "  http://<FLOATING_IP>/\n",
    "Upload a sequence JSON or image input (depending on model).\n",
    "\n",
    "Jupyter Notebook:\n",
    "  docker logs jupyter\n",
    "\n",
    "Open:\n",
    "  http://<YOUR_FLOATING_IP>:8888/lab?token=...\n",
    "Open notebook: work/triton.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark Inference via Triton Client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tritonclient.http import InferenceServerClient, InferInput\n",
    "import numpy as np\n",
    "\n",
    "client = InferenceServerClient(\"localhost:8000\")\n",
    "input_user = InferInput(\"USER_ID\", [1, 1], \"INT64\")\n",
    "input_seq  = InferInput(\"SEQ\", [1, 50], \"INT64\")\n",
    "\n",
    "input_user.set_data_from_numpy(np.array([[42]], dtype=np.int64))\n",
    "input_seq.set_data_from_numpy(np.random.randint(0, 100, size=(1, 50), dtype=np.int64))\n",
    "\n",
    "outputs = [InferRequestedOutput(\"TOP_K\")]\n",
    "\n",
    "res = client.infer(\"recommender_model\", [input_user, input_seq], outputs=outputs)\n",
    "print(res.as_numpy(\"TOP_K\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serving a PyTorch model\n",
    "\n",
    "The Triton client comes with a performance analyzer, which we can use to send requests to the server and get some statistics back. Let’s try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_analyzer -u triton_server:8000 -m recommender_model --input-data input.json -b 1 --concurrency-range 8\n",
    "perf_analyzer -u triton_server:8000 -m recommender_model --input-data input.json -b 1 --concurrency-range 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a note of the line showing the total average request latency, and the breakdown including:\n",
    "\n",
    "-   `queue`, the queuing delay\n",
    "-   and `compute infer`, the inference delay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable Dynamic Batching\n",
    "Edit model config:\n",
    "dynamic_batching {\n",
    "  preferred_batch_size: [4, 6, 8, 10]\n",
    "  max_queue_delay_microseconds: 100\n",
    "}\n",
    "\n",
    "Then rebuild and restart:\n",
    "\n",
    "docker compose -f ~/serve-system-chi/docker/docker-compose-triton.yaml build triton_server\n",
    "docker compose -f ~/serve-system-chi/docker/docker-compose-triton.yaml up triton_server --force-recreate -d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Instance GPU Scaling\n",
    "Edit model config:\n",
    "instance_group [\n",
    "  { count: 2 kind: KIND_GPU gpus: [ 0 ] },\n",
    "  { count: 2 kind: KIND_GPU gpus: [ 1 ] }\n",
    "]\n",
    "\n",
    "scale up to:\n",
    "instance_group [\n",
    "  { count: 4 kind: KIND_GPU gpus: [ 0 ] },\n",
    "  { count: 4 kind: KIND_GPU gpus: [ 1 ] }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_analyzer -u triton_server:8000 -m recommender_model --input-data input.json -b 1 --concurrency-range 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Migrate to ONNX Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_analyzer -u triton_server:8000 -m recommender_model_onnx -b 1 --shape USER_ID:1, SEQ:50 --concurrency-range 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flask + ONNX Integration\n",
    "context: https://github.com/teaching-on-testbeds/gourmetgram.git#triton_onnx\n",
    "environment:\n",
    "  - MODEL_NAME=recommender_model_onnx\n",
    "\n",
    "\n",
    "docker compose -f ~/serve-system-chi/docker/docker-compose-triton.yaml build flask\n",
    "docker compose -f ~/serve-system-chi/docker/docker-compose-triton.yaml up flask --force-recreate -d\n",
    "\n",
    "Test Flask UI:\n",
    "http://<FLOATING_IP>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
